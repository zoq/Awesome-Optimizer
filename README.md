# Awesome-Optimizer

A collection of optimizer-related papers and code.

For the last column, we let `GD` for Gradient Descent, `S` for second-order (quasi-newton) methods, `E` for evolutionary, `GF` for gradient free, `VR` for variance reduced.

| Title                                                                                                                      | Year |       Optimizer       | Published                                                                                                                                                      | Code                                                                                                                                                                 |      |
| -------------------------------------------------------------------------------------------------------------------------- | ---- | :-------------------: | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- |
| AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix                            | 2023 |          AGD          | [neurips'23](https://arxiv.org/abs/2312.01658)                                                                                                                 | [pytorch](https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers)                                                              | GD,S |
| AdaLomo: Low-memory Optimization with Adaptive Learning Rate                                                               | 2023 |        AdaLOMO        | [arxiv](https://arxiv.org/abs/2310.10195)                                                                                                                      | [pytorch](https://github.com/OpenLMLab/LOMO)                                                                                                                         | GD   |
| Large Language Models as Optimizers                                                                                        | 2023 |         OPRO          | [arxiv](https://arxiv.org/abs/2309.03409)                                                                                                                      | [python](https://github.com/google-deepmind/opro)                                                                                                                    | llm  |
| Promoting Exploration in Memory-Augmented Adam using Critical Momenta                                                      | 2023 |        Adam+CM        | [arxiv](https://arxiv.org/abs/2307.09638)                                                                                                                      | [pytorch](https://github.com/chandar-lab/CMOptimizer)                                                                                                                | GD   |
| CAME: Confidence-guided Adaptive Memory Efficient Optimization                                                             | 2023 |         CAME          | [acl'23](https://arxiv.org/abs/2307.02047)                                                                                                                     | [pytorch](https://github.com/yangluo7/CAME)                                                                                                                          | GD   |
| Full Parameter Fine-tuning for Large Language Models with Limited Resources                                                | 2023 |         LOMO          | [arxiv](https://arxiv.org/abs/2306.09782)                                                                                                                      | [pytorch](https://github.com/OpenLMLab/LOMO)                                                                                                                         | GD   |
| Prodigy: An Expeditiously Adaptive Parameter-Free Learner                                                                  | 2023 |        Prodigy        | [arxiv](https://arxiv.org/abs/2306.06101)                                                                                                                      | [pytorch](https://github.com/konstmish/prodigy)                                                                                                                      | GD   |
| DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method                                              | 2023 |         DoWG          | [neurips'23](https://arxiv.org/abs/2305.16284)                                                                                                                 |                                                                                                                                                                      | GD   |
| Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training                                       | 2023 |      **Sophia**       | [arxiv](https://arxiv.org/abs/2305.14342)                                                                                                                      | [pytorch](https://github.com/Liuhong99/Sophia)                                                                                                                       | GD   |
| UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization                                      | 2023 |         UAdam         | [arxiv](https://arxiv.org/abs/2305.05675)                                                                                                                      |                                                                                                                                                                      | GD   |
| Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term                                        | 2023 |         WSAM          | [kdd'23](https://arxiv.org/abs/2305.15817)                                                                                                                     | [pytorch](https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers)                                                              | GD   |
| DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation                                                             | 2023 |        DP-Adam        | [iclr-W'23](https://arxiv.org/abs/2304.11208)                                                                                                                  |                                                                                                                                                                      | GD   |
| An Adam-enhanced Particle Swarm Optimizer for Latent Factor Analysis                                                       | 2023 |         ADHPL         | [arxiv](https://arxiv.org/abs/2302.11956)                                                                                                                      |                                                                                                                                                                      | E    |
| DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule                                                      | 2023 |          DoG          | [icml'23](https://arxiv.org/abs/2302.12022)                                                                                                                    | [pytorch](https://github.com/formll/dog)                                                                                                                             | GD   |
| FOSI: Hybrid First and Second Order Optimization                                                                           | 2023 |         FOSI          | [HPI'23](https://arxiv.org/abs/2302.08484)                                                                                                                     | [jax](https://github.com/hsivan/fosi)                                                                                                                                | GD,S |
| Symbolic Discovery of Optimization Algorithms                                                                              | 2023 |       **Lion**        | [neurips'23](https://arxiv.org/abs/2302.06675)                                                                                                                 | [jax, tf, pytorch](https://github.com/google/automl/tree/master/lion)                                                                                                | GD   |
| Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale                                      | 2022 |       **Amos**        | [arxiv](https://arxiv.org/pdf/2210.11693.pdf)                                                                                                                  | [jax](https://github.com/google-research/jestimator)                                                                                                                 | GD   |
| VeLO: Training Versatile Learned Optimizers by Scaling Up                                                                  | 2022 |       **VeLO**        | [arxiv](https://arxiv.org/abs/2211.09760)                                                                                                                      | [jax](https://github.com/google/learned_optimization/tree/main/learned_optimization/research/general_lopt)                                                           | GD   |
| Grad-GradaGrad? A Non-Monotone Adaptive Stochastic Gradient Method                                                         | 2022 |       GradaGrad       | [arxiv](https://arxiv.org/abs/2206.06900)                                                                                                                      |                                                                                                                                                                      | GD   |
| CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU                                  | 2022 |        CowClip        | [aaai'23](https://arxiv.org/pdf/2204.06240.pdf)                                                                                                                | [tf](https://github.com/bytedance/LargeBatchCTR)                                                                                                                     | GD   |
| Smooth momentum: improving lipschitzness in gradient descent                                                               | 2022 |    Smooth Momentum    | [APIN](https://link.springer.com/article/10.1007/s10489-022-04207-7)                                                                                           |                                                                                                                                                                      | GD   |
| Towards Better Generalization of Adaptive Gradient Methods                                                                 | 2020 |         SAGD          | [neurips'20](https://proceedings.neurips.cc/paper/2020/hash/08fb104b0f2f838f3ce2d2b3741a12c2-Abstract.html)                                                    |                                                                                                                                                                      | GD   |
| An Improved Adaptive Optimization Technique for Image Classification                                                       | 2020 |       Mean-ADAM       | [ICIEV](https://ieeexplore.ieee.org/abstract/document/9306620)                                                                                                 |                                                                                                                                                                      | GD   |
| SCW-SGD: Stochastically Confidence-Weighted SGD                                                                            | 2020 |        SCWSGD         | [ICIP](https://ieeexplore.ieee.org/abstract/document/9190992)                                                                                                  |                                                                                                                                                                      | GD   |
| Slime mould algorithm: A new method for stochastic optimization                                                            | 2020 |          SMA          | [FGCS](https://www.sciencedirect.com/science/article/abs/pii/S0167739X19320941)                                                                                | [code](http://mdm.wzu.edu.cn/SMA.html)                                                                                                                               | E    |
| Ranger-Deep-Learning-Optimizer                                                                                             | 2020 |        Ranger         | [github](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)                                                                                          | [pytorch](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)                                                                                               | GD   |
| pbSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization                                 | 2020 |         pbSGD         | [ijcai'20](https://www.ijcai.org/proceedings/2020/451)                                                                                                         | [pytorch](https://github.com/HAIRLAB/pbSGD)                                                                                                                          | GD   |
| A Variant of Gradient Descent Algorithm Based on Gradient Averaging                                                        | 2020 |       Grad-Avg        | [arxiv](https://arxiv.org/abs/2012.02387)                                                                                                                      |                                                                                                                                                                      | GD   |
| Stochastic Gradient Descent with Nonlinear Conjugate Gradient-Style Adaptive Momentum                                      | 2020 |         FRSGD         | [arxiv](https://arxiv.org/abs/2012.02188)                                                                                                                      |                                                                                                                                                                      | GD   |
| CADA: Communication-Adaptive Distributed Adam                                                                              | 2020 |         CADA          | [arxiv](https://arxiv.org/abs/2012.15469)                                                                                                                      | [pytorch, matlab](https://github.com/ChrisYZZ/CADA-master)                                                                                                           | GD   |
| Eigenvalue-corrected Natural Gradient Based on a New Approximation                                                         | 2020 |        TEKFAC         | [arxiv](https://arxiv.org/abs/2011.13609)                                                                                                                      |                                                                                                                                                                      | GD   |
| SMG: A Shuffling Gradient-Based Method with Momentum                                                                       | 2020 |          SMG          | [icml'21](https://arxiv.org/abs/2011.11884)                                                                                                                    |                                                                                                                                                                      | GD   |
| SALR: Sharpness-aware Learning Rate Scheduler for Improved Generalization                                                  | 2020 |         SALR          | [TNNLS](https://arxiv.org/abs/2011.05348)                                                                                                                      |                                                                                                                                                                      | GD   |
| Self-Tuning Stochastic Optimization with Curvature-Aware Gradient Filtering                                                | 2020 |         MEKA          | [neurips-W'21](https://arxiv.org/abs/2011.04803)                                                                                                               |                                                                                                                                                                      | GD   |
| Mixing ADAM and SGD: a Combined Optimization Method                                                                        | 2020 |          MAS          | [arxiv](https://arxiv.org/abs/2011.08042)                                                                                                                      | [pytorch](https://gitlab.com/nicolalandro/multi_optimizer)                                                                                                           | GD   |
| EAdam Optimizer: How Îµ Impact Adam                                                                                         | 2020 |         EAdam         | [arxiv](https://arxiv.org/abs/2011.02150)                                                                                                                      | [pytorch](https://github.com/yuanwei2019/EAdam-optimizer)                                                                                                            | GD   |
| Adam<sup>+</sup>: A Stochastic Method with Adaptive Variance Reduction                                                     | 2020 |   Adam<sup>+</sup>    | [arxiv](https://arxiv.org/abs/2011.11985)                                                                                                                      |                                                                                                                                                                      | GD   |
| Sharpness-aware Minimization for Efficiently Improving Generalization                                                      | 2020 |        **SAM**        | [iclr'21](https://arxiv.org/abs/2010.01412)                                                                                                                    | [jax](https://github.com/google-research/sam)                                                                                                                        | GD   |
| Expectigrad: Fast Stochastic Optimization with Robust Convergence Properties                                               | 2020 |      Expectigrad      | [arxiv](https://arxiv.org/abs/2010.01356)                                                                                                                      | [tf](https://github.com/brett-daley/expectigrad)                                                                                                                     | GD   |
| AEGD: Adaptive Gradient Descent with Energy                                                                                | 2020 |         AEGD          | [AIMS](https://arxiv.org/abs/2010.05109)                                                                                                                       | [pytorch](https://github.com/txping/AEGD)                                                                                                                            | GD   |
| Adam with Bandit Sampling for Deep Learning                                                                                | 2020 |        Adambs         | [arxiv](https://arxiv.org/abs/2010.12986)                                                                                                                      |                                                                                                                                                                      | GD   |
| AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients                                                | 2020 |     **AdaBelief**     | [neurips'20](https://arxiv.org/abs/2010.07468)                                                                                                                 | [pytorch](https://github.com/juntang-zhuang/Adabelief-Optimizer)                                                                                                     | GD   |
| Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization                      | 2020 |       Apollo[W]       | [arxiv](https://arxiv.org/abs/2009.13586)                                                                                                                      | [pytorch](https://github.com/XuezheMax/apollo)                                                                                                                       | GD,S |
| S-SGD: Symmetrical Stochastic Gradient Descent with Weight Noise Injection for Reaching Flat Minima                        | 2020 |         S-SGD         | [arxiv](https://arxiv.org/abs/2009.02479)                                                                                                                      |                                                                                                                                                                      | GD   |
| Gravilon: Applications of a New Gradient Descent Method to Machine Learning                                                | 2020 |       Gravilon        | [arxiv](https://arxiv.org/abs/2008.11370)                                                                                                                      |                                                                                                                                                                      | GD   |
| PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization                                     | 2020 |         PAGE          | [icml'21](https://arxiv.org/abs/2008.10898)                                                                                                                    |                                                                                                                                                                      | GD   |
| Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities                                 | 2020 |    Ada{ACSA,AGD+}     | [aaai'21](https://arxiv.org/abs/2007.08840)                                                                                                                    |                                                                                                                                                                      | GD   |
| Stochastic Normalized Gradient Descent with Momentum for Large Batch Training                                              | 2020 |         SNGM          | [arxiv](https://arxiv.org/abs/2007.13985)                                                                                                                      |                                                                                                                                                                      | GD   |
| AdaScale SGD: A User-Friendly Algorithm for Distributed Training                                                           | 2020 |       AdaScale        | [icml'21](https://arxiv.org/abs/2007.05105)                                                                                                                    |                                                                                                                                                                      | GD   |
| Momentum-based variance-reduced proximal stochastic gradient method for composite nonconvex stochastic optimization        | 2020 |        PSTorm         | [JOTA](https://arxiv.org/abs/2006.00425)                                                                                                                       |                                                                                                                                                                      | GD   |
| MTAdam: Automatic Balancing of Multiple Training Loss Terms                                                                | 2020 |        MTAdam         | [acl'21](https://arxiv.org/abs/2006.14683)                                                                                                                     | [pytorch](https://github.com/ItzikMalkiel/MTAdam)                                                                                                                    | GD   |
| AdaSGD: Bridging the gap between SGD and Adam                                                                              | 2020 |        AdaSGD         | [arxiv](https://arxiv.org/abs/2006.16541)                                                                                                                      |                                                                                                                                                                      | GD   |
| AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights                                        | 2020 |         AdamP         | [iclr'21](https://arxiv.org/abs/2006.08217)                                                                                                                    | [pytorch](https://github.com/clovaai/AdamP)                                                                                                                          | GD   |
| Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes                                                     | 2020 |         LANS          | [arxiv](https://arxiv.org/abs/2006.13484)                                                                                                                      | [pytorch](https://github.com/jianyuheng/lans_optimizer)                                                                                                              | GD   |
| AdaSwarm: Augmenting Gradient-Based optimizers in Deep Learning with Swarm Intelligence                                    | 2020 |       AdaSwarm        | [TETC](https://arxiv.org/abs/2006.09875)                                                                                                                       | [pytorch](https://github.com/AdaSwarm/AdaSwarm)                                                                                                                      | E    |
| Enhance Curvature Information by Structured Stochastic Quasi-Newton Methods                                                | 2020 |       SKQN,S4QN       | [cvpr'21](https://arxiv.org/abs/2006.09606)                                                                                                                    |                                                                                                                                                                      | GD   |
| Adaptive Gradient Methods Can Be Provably Faster than SGD after Finite Epochs                                              | 2020 |       SHAdaGrad       | [arxiv](https://arxiv.org/abs/2006.07037)                                                                                                                      |                                                                                                                                                                      | GD   |
| A New Accelerated Stochastic Gradient Method with Momentum                                                                 | 2020 |         SGDM          | [arxiv](https://arxiv.org/abs/2006.00423)                                                                                                                      |                                                                                                                                                                      | GD   |
| Practical Quasi-Newton Methods for Training Deep Neural Networks                                                           | 2020 |    **K-BFGS**[(L)]    | [neurips'20](https://arxiv.org/abs/2006.08877)                                                                                                                 | [pytorch](https://github.com/renyiryry/kbfgs_neurips2020_public)                                                                                                     | GD   |
| AdaS: Adaptive Scheduling of Stochastic Gradients                                                                          | 2020 |         AdaS          | [cvpr'22](https://arxiv.org/abs/2006.06587)                                                                                                                    | [pytorch](https://github.com/mahdihosseini/AdaS)                                                                                                                     | GD   |
| Adai: Separating the Effects of Adaptive Learning Rate and Momentum Inertia                                                | 2020 |         Adai          | [icml'22](https://arxiv.org/abs/2006.15815)                                                                                                                    | [pytorch](https://github.com/zeke-xie/adaptive-inertia-adai)                                                                                                         | GD   |
| ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning                                                        | 2020 |      ADAHESSIAN       | [aaai'21](https://arxiv.org/abs/2006.00719)                                                                                                                    | [pytorch](https://github.com/amirgholami/adahessian)                                                                                                                 | GD   |
| Momentum with Variance Reduction for Nonconvex Composition Optimization                                                    | 2020 |      MVRC-[1,2]       | [arxiv](https://arxiv.org/abs/2005.07755)                                                                                                                      |                                                                                                                                                                      | GD   |
| CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics with Simulated Annealing                           | 2020 |     CoolMomentum      | [arxiv](https://arxiv.org/abs/2005.14605)                                                                                                                      | [tf, pytorch](https://github.com/borbysh/coolmomentum)                                                                                                               | GD   |
| Gradient Centralization: A New Optimization Technique for Deep Neural Networks                                             | 2020 |          GC           | [eccv'20](https://arxiv.org/abs/2004.01461)                                                                                                                    | [pytorch, tf](https://github.com/Yonghongwei/Gradient-Centralization)                                                                                                | GD   |
| AdaX: Adaptive Gradient Descent with Exponential Long Term Memory                                                          | 2020 |       AdaX[-W]        | [arxiv](https://arxiv.org/abs/2004.09740)                                                                                                                      | [pytorch](https://github.com/switchablenorms/AdaX)                                                                                                                   | GD   |
| Weak and Strong Gradient Directions: Explaining Memorization, Generalization, and Hardness of Examples at Scale            | 2020 |          RM3          | [arxiv](https://arxiv.org/abs/2003.07422)                                                                                                                      | [tf](https://github.com/cpuimage/keras-optimizer)                                                                                                                    | GD   |
| TAdam: A Robust Stochastic Gradient Optimizer                                                                              | 2020 |         TAdam         | [arxiv](https://arxiv.org/abs/2003.00179)                                                                                                                      | [pytorch](https://github.com/Mahoumaru/TAdam)                                                                                                                        | GD   |
| Iterative Averaging in the Quest for Best Test Error                                                                       | 2020 |         Gadam         | [arxiv](https://arxiv.org/abs/2003.01247)                                                                                                                      |                                                                                                                                                                      | GD   |
| On the distance between two neural networks and the stability of learning                                                  | 2020 |        Fromage        | [neurips'20](https://arxiv.org/abs/2002.03432)                                                                                                                 | [pytorch](https://github.com/jxbz/fromage)                                                                                                                           | GD   |
| Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent                                                     | 2020 |         SRSGD         | [arxiv](https://arxiv.org/abs/2002.10583)                                                                                                                      | [pytorch](https://github.com/minhtannguyen/SRSGD)                                                                                                                    | GD   |
| Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent                                             | 2020 |        SGD-G2         | [arxiv](https://arxiv.org/abs/2002.09304)                                                                                                                      |                                                                                                                                                                      | GD   |
| LaProp: Separating Momentum and Adaptivity in Adam                                                                         | 2020 |        LaProp         | [arxiv](https://arxiv.org/abs/2002.04839)                                                                                                                      | [pytorch](https://github.com/Z-T-WANG/LaProp-Optimizer)                                                                                                              | GD   |
| Compositional ADAM: An Adaptive Compositional Solver                                                                       | 2020 |        C-ADAM         | [arxiv](https://arxiv.org/abs/2002.03755)                                                                                                                      |                                                                                                                                                                      | GD   |
| Biased Stochastic Gradient Descent for Conditional Stochastic Optimization                                                 | 2020 |         BSGD          | [arxiv](https://arxiv.org/abs/2002.10790)                                                                                                                      |                                                                                                                                                                      | GD   |
| On the Trend-corrected Variant of Adaptive Stochastic Optimization Methods                                                 | 2020 |         AdamT         | [ijcnn'20](https://arxiv.org/abs/2001.06130)                                                                                                                   | [pytorch](https://github.com/xuebin-zh/AdamT)                                                                                                                        | GD   |
| Efficient Learning Rate Adaptation for Convolutional Neural Network Training                                               | 2019 |        e-AdLR         | [ijcnn'19](https://ieeexplore.ieee.org/abstract/document/8852033)                                                                                              |                                                                                                                                                                      | GD   |
| ProxSGD: Training Structured Neural Networks under Regularization and Constraints                                          | 2019 |        ProxSGD        | [iclr'20](https://openreview.net/forum?id=HygpthEtvr)                                                                                                          | [tf](https://github.com/optyang/proxsgd)                                                                                                                             | GD   |
| An Adaptive Optimization Algorithm Based on Hybrid Power and Multidimensional Update Strategy                              | 2019 |        AdaHMG         | [ieee](https://ieeexplore.ieee.org/abstract/document/8635473)                                                                                                  |                                                                                                                                                                      | GD   |
| signSGD via Zeroth-Order Oracle                                                                                            | 2019 |      ZO-signSGD       | [iclr'19](https://openreview.net/forum?id=BJe-DsC5Fm)                                                                                                          |                                                                                                                                                                      | GF   |
| Fast DENSER: Efficient Deep NeuroEvolution                                                                                 | 2019 |       F-DENSER        | [arxiv](https://arxiv.org/abs/1905.02969)                                                                                                                      | [tf](https://github.com/fillassuncao/fast-denser)                                                                                                                    | E    |
| Adathm: Adaptive Gradient Method Based on Estimates of Third-Order Moments                                                 | 2019 |        Adathm         | [DSC](https://ieeexplore.ieee.org/document/8923615)                                                                                                            |                                                                                                                                                                      | GD   |
| A new perspective in understanding of Adam-Type algorithms and beyond                                                      | 2019 |        AdamAL         | [arxiv](https://openreview.net/forum\?id\=SyxM51BYPB)                                                                                                          | [pytorch](https://www.dropbox.com/s/qgqhg6znuimzci9/adamAL.py\?dl\=0)                                                                                                | GD   |
| CProp: Adaptive Learning Rate Scaling from Past Gradient Conformity                                                        | 2019 |         CProp         | [arxiv](https://arxiv.org/abs/1912.11493)                                                                                                                      | [pytorch](https://github.com/phizaz/cprop)                                                                                                                           | GD   |
| Domain-independent Dominance of Adaptive Methods                                                                           | 2019 | AvaGrad, Delayed Adam | [cvpr'21](https://arxiv.org/abs/1912.01823)                                                                                                                    | [pytorch](https://github.com/lolemacs/avagrad)                                                                                                                       | GD   |
| Second-order Information in First-order Optimization Methods                                                               | 2019 |        AdaSqrt        | [arxiv](https://arxiv.org/abs/1912.09926)                                                                                                                      | [tf](https://github.com/OSI-Group/AdaSqrt)                                                                                                                           | GD   |
| Does Adam optimizer keep close to the optimal point?                                                                       | 2019 |        AdaFix         | [arxiv](https://arxiv.org/abs/1911.00289)                                                                                                                      |                                                                                                                                                                      | GD   |
| Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates                           | 2019 |       AdaAlter        | [arxiv](https://arxiv.org/abs/1911.09030)                                                                                                                      | [mxnet](https://github.com/xcgoner/AISTATS2020-AdaAlter-GluonNLP)                                                                                                    | GD   |
| UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization                             | 2019 |       UniXGrad        | [neurips'19](https://arxiv.org/abs/1910.13857)                                                                                                                 |                                                                                                                                                                      | GD   |
| Demon: Improved Neural Network Training with Momentum Decay                                                                | 2019 |   Demon {SGDM,Adam}   | [icassp'22](https://arxiv.org/abs/1910.04952)                                                                                                                  | [tf](https://github.com/autasi/demon_sgd)                                                                                                                            | GD   |
| ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization                                                 | 2019 |       ZO-AdaMM        | [neurips'19](https://arxiv.org/abs/1910.06513)                                                                                                                 | [tf](https://github.com/KaidiXu/ZO-AdaMM)                                                                                                                            | GF   |
| On Empirical Comparisons of Optimizers for Deep Learning                                                                   | 2019 |       RMSterov        | [arxiv](https://arxiv.org/abs/1910.05446)                                                                                                                      |                                                                                                                                                                      | GD   |
| An Adaptive and Momental Bound Method for Stochastic Learning                                                              | 2019 |        AdaMod         | [arxiv](https://arxiv.org/abs/1910.12249)                                                                                                                      | [pytorch](https://github.com/lancopku/AdaMod)                                                                                                                        | GD   |
| On Higher-order Moments in Adam                                                                                            | 2019 |         HAdam         | [arxiv](https://arxiv.org/abs/1910.06878)                                                                                                                      |                                                                                                                                                                      | GD   |
| diffGrad: An Optimization Method for Convolutional Neural Networks                                                         | 2019 |       diffGrad        | [TNNLS](https://arxiv.org/abs/1909.11015)                                                                                                                      | [pytorch](https://github.com/shivram1987/diffGrad)                                                                                                                   | GD   |
| Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM                                                      | 2019 |       SAMSGrad        | [arxiv](https://arxiv.org/abs/1908.00700)                                                                                                                      | [pytorch](https://github.com/neilliang90/Sadam)                                                                                                                      | GD   |
| On the Variance of the Adaptive Learning Rate and Beyond                                                                   | 2019 |       **RAdam**       | [iclr'20](https://arxiv.org/abs/1908.03265)                                                                                                                    | [pytorch, TF](https://github.com/LiyuanLucasLiu/RAdam)                                                                                                               | GD   |
| BGADAM: Boosting based Genetic-Evolutionary ADAM for Neural Network Optimization                                           | 2019 |        BGADAM         | [arxiv](https://arxiv.org/abs/1908.08015)                                                                                                                      |                                                                                                                                                                      | GD   |
| Adaloss: Adaptive Loss Function for Landmark Localization                                                                  | 2019 |        Adaloss        | [arxiv](https://arxiv.org/abs/1908.01070)                                                                                                                      |                                                                                                                                                                      | GD   |
| signADAM: Learning Confidences for Deep Neural Networks                                                                    | 2019 |     signADAM[++]      | [icdmw'19](https://arxiv.org/abs/1907.09008)                                                                                                                   | [pytorch](https://github.com/DongWanginxdu/signADAM-Learn-by-Confidence)                                                                                             | GD   |
| The Role of Memory in Stochastic Optimization                                                                              | 2019 |       PolyAdam        | [UAI'20](https://arxiv.org/abs/1907.01678)                                                                                                                     |                                                                                                                                                                      | GD   |
| Lookahead Optimizer: k steps forward, 1 step back                                                                          | 2019 |     **Lookahead**     | [neurips'19](https://arxiv.org/abs/1907.08610)                                                                                                                 | [tf, pytorch](https://github.com/michaelrzhang/lookahead)                                                                                                            | GD   |
| Momentum-Based Variance Reduction in Non-Convex SGD                                                                        | 2019 |         STORM         | [neurips'19](https://arxiv.org/abs/1905.10018)                                                                                                                 | [pytorch](https://github.com/darshank528/Project-STORM)                                                                                                              | GD   |
| SAdam: A Variant of Adam for Strongly Convex Functions                                                                     | 2019 |         SAdam         | [iclr'20](https://arxiv.org/abs/1905.02957)                                                                                                                    | [code](https://github.com/SAdam-iclr2020/codes)                                                                                                                      | GD   |
| Matrix-Free Preconditioning in Online Learning                                                                             | 2019 |  RecursiveOptimizer   | [icml'19](https://arxiv.org/abs/1905.12721)                                                                                                                    | [tf](https://github.com/google-research/google-research/tree/master/recursive_optimizer)                                                                             | GD   |
| PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization                                             | 2019 |      PowerSGD[M]      | [neurips'19](https://arxiv.org/abs/1905.13727)                                                                                                                 | [pytorch](https://github.com/epfml/powersgd)                                                                                                                         | GD   |
| Fast-DENSER++: Evolving Fully-Trained Deep Artificial Neural Networks                                                      | 2019 |      F-DENSER++       | [arxiv](https://arxiv.org/abs/1905.02969)                                                                                                                      | [tf](https://github.com/fillassuncao/fast-denser)                                                                                                                    | E    |
| Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks                                 | 2019 |       Novograd        | [neurips'19](https://arxiv.org/abs/1905.11286)                                                                                                                 | [pytorch](https://github.com/convergence-lab/novograd)                                                                                                               | GD   |
| An Adaptive Remote Stochastic Gradient Method for Training Neural Networks                                                 | 2019 |    NAMS{G,B},ARSG     | [arxiv](https://arxiv.org/abs/1905.01422)                                                                                                                      | [pytorch,mxnet](https://github.com/rationalspark/NAMSG)                                                                                                              | GD   |
| Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates                                            | 2019 |       ArmijoLS        | [neurips'19](https://arxiv.org/abs/1905.09997)                                                                                                                 | [pytorch](https://github.com/IssamLaradji/sls)                                                                                                                       | GD   |
| Large Batch Optimization for Deep Learning: Training BERT in 76 minutes                                                    | 2019 |       **LAMB**        | [iclr'19](https://arxiv.org/abs/1904.00962)                                                                                                                    | [tf](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py),[pytorch](https://github.com/NUS-HPC-AI-Lab/pytorch-lamb)                | GD   |
| On the Convergence Proof of AMSGrad and a New Version                                                                      | 2019 |         AdamX         | [arxiv](https://arxiv.org/abs/1904.03590)                                                                                                                      |                                                                                                                                                                      | GD   |
| An Optimistic Acceleration of AMSGrad for Nonconvex Optimization                                                           | 2019 |      OPT-AMSGrad      | [acml'21](https://arxiv.org/abs/1903.01435)                                                                                                                    |                                                                                                                                                                      | GD   |
| Parabolic Approximation Line Search for DNNs                                                                               | 2019 |          PAL          | [neurip'20](https://arxiv.org/abs/1903.11991)                                                                                                                  | [pytorch](https://github.com/cogsys-tuebingen/PAL)                                                                                                                   | GD   |
| Gradient-only line searches: An Alternative to Probabilistic Line Searches                                                 | 2019 |        GOLS-I         | [arxiv](https://arxiv.org/abs/1903.09383)                                                                                                                      |                                                                                                                                                                      | GD   |
| Adaptive Gradient Methods with Dynamic Bound of Learning Rate                                                              | 2019 |       AdaBound        | [iclr'19](https://arxiv.org/abs/1902.09843)                                                                                                                    | [pytorch](https://github.com/Luolc/AdaBound)                                                                                                                         | GD   |
| Memory-Efficient Adaptive Optimization                                                                                     | 2019 |          SM3          | [neurips'19](https://arxiv.org/abs/1901.11150)                                                                                                                 | [tf](https://github.com/google-research/google-research/tree/master/sm3)                                                                                             | GD   |
| DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online Optimization                                      | 2019 |         DADAM         | [arxiv](https://arxiv.org/abs/1901.09109)                                                                                                                      | [matlab](https://github.com/Tarzanagh/DADAM)                                                                                                                         | GD   |
| On the Convergence of AdaGrad with Momentum for Training Deep Neural Networks                                              | 2018 |      Ada{NAG,HB}      | [arxiv](https://deepai.org/publication/on-the-convergence-of-adagrad-with-momentum-for-training-deep-neural-networks)                                          |                                                                                                                                                                      | GD   |
| SADAGRAD: Strongly Adaptive Stochastic Gradient Methods                                                                    | 2018 |       SADAGRAD        | [icml'18](http://proceedings.mlr.press/v80/chen18m.html)                                                                                                       |                                                                                                                                                                      | GD   |
| PSA-CMA-ES: CMA-ES with population size adaptation                                                                         | 2018 |      PSA-CMA-ES       | [gecco'18](https://dl.acm.org/doi/10.1145/3205455.3205467)                                                                                                     |                                                                                                                                                                      | E    |
| Adaptive Methods for Nonconvex Optimization                                                                                | 2018 |         Yogi          | [neurips'18](https://papers.nips.cc/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html)                                                            | [tf](https://github.com/4rtemi5/Yogi-Optimizer_Keras)                                                                                                                | GD   |
| Deep Frank-Wolfe For Neural Network Optimization                                                                           | 2018 |          DFW          | [iclr'19](https://arxiv.org/abs/1811.07591)                                                                                                                    | [pytorch](https://github.com/oval-group/dfw)                                                                                                                         | GD   |
| HyperAdam: A Learnable Task-Adaptive Adam for Network Training                                                             | 2018 |       HyperAdam       | [aaai'19](https://arxiv.org/abs/1811.08996)                                                                                                                    | [tf, pytorch](https://github.com/ShipengWang/HyperAdam)                                                                                                              | GD   |
| Practical Bayesian Learning of Neural Networks via Adaptive Optimisation Methods                                           | 2018 |         BADAM         | [icml'20](https://arxiv.org/abs/1811.03679)                                                                                                                    | [tf](https://github.com/skezle/BADAM)                                                                                                                                | GD   |
| Kalman Gradient Descent: Adaptive Variance Reduction in Stochastic Optimization                                            | 2018 |          KGD          | [arxiv](https://arxiv.org/abs/1810.12273)                                                                                                                      | [tf](https://github.com/jamesvuc/KGD)                                                                                                                                | GD   |
| Quasi-hyperbolic momentum and Adam for deep learning                                                                       | 2018 |      QHM,QHAdam       | [iclr'19](https://arxiv.org/abs/1810.06801)                                                                                                                    | [pytorch, tf](https://github.com/facebookresearch/qhoptim)                                                                                                           | GD   |
| AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods                                                  | 2018 |       AdaShift        | [iclr'19](https://arxiv.org/abs/1810.00143)                                                                                                                    | [pytorch](https://github.com/MichaelKonobeev/adashift)                                                                                                               | GD   |
| Optimal Adaptive and Accelerated Stochastic Gradient Descent                                                               | 2018 |  A2Grad{Exp,Inc,Uni}  | [arxiv](https://arxiv.org/abs/1810.00553)                                                                                                                      | [pytorch](https://github.com/severilov/A2Grad_optimizer)                                                                                                             | GD   |
| Accelerating SGD with momentum for over-parameterized learning                                                             | 2018 |         MaSS          | [arxiv](https://arxiv.org/abs/1810.13395)                                                                                                                      | [tf](https://github.com/ts66395/MaSS)                                                                                                                                | GD   |
| Online Adaptive Methods, Universality and Acceleration                                                                     | 2018 |      AcceleGrad       | [neurips'18](https://arxiv.org/abs/1809.02864)                                                                                                                 |                                                                                                                                                                      | GD   |
| On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization                                          | 2018 |        AdaFom         | [iclr'19](https://arxiv.org/abs/1808.02941)                                                                                                                    |                                                                                                                                                                      | GD   |
| AdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes                                                             | 2018 |     AdaGrad-Norm      | [icml'19](https://arxiv.org/abs/1806.01811)                                                                                                                    | [pytorch](https://github.com/xwuShirley/pytorch/blob/master/torch/optim/adagradnorm.py)                                                                              | GD   |
| Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam                                                    | 2018 |         VAdam         | [vadam'18](https://arxiv.org/abs/1806.04854)                                                                                                                   | [pytorch, tf](https://github.com/emtiyaz/vadam)                                                                                                                      | GD   |
| Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks                               | 2018 |         Padam         | [ijcai'20](https://arxiv.org/abs/1806.06763)                                                                                                                   | [pytorch](https://github.com/uclaml/Padam)                                                                                                                           | GD   |
| Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis                                               | 2018 |         EKFAC         | [neurips'18](https://arxiv.org/abs/1806.03884)                                                                                                                 | [pytorch](https://github.com/Thrandis/EKFAC-pytorch)                                                                                                                 | GD   |
| Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods                                   | 2018 |     AdaBayes[FP]      | [neurips'18](https://arxiv.org/abs/1807.07540)                                                                                                                 | [pytorch](https://github.com/LaurenceA/adabayes)                                                                                                                     | GD   |
| Nostalgic Adam: Weighting more of the past gradients when designing the adaptive learning rate                             | 2018 |        NosAdam        | [ijcai'19](https://arxiv.org/abs/1805.07557)                                                                                                                   | [pytorch](https://github.com/andrehuang/NostalgicAdam-NosAdam)                                                                                                       | GD   |
| Small steps and giant leaps: Minimal Newton solvers for Deep Learning                                                      | 2018 |       Curveball       | [iccv'19](https://arxiv.org/abs/1805.08095)                                                                                                                    | [matlab](https://github.com/jotaf98/curveball)                                                                                                                       | GD   |
| GADAM: Genetic-Evolutionary ADAM for Deep Neural Network Optimization                                                      | 2018 |         GADAM         | [arxiv](https://arxiv.org/abs/1805.07500)                                                                                                                      |                                                                                                                                                                      | GD   |
| Adafactor: Adaptive Learning Rates with Sublinear Memory Cost                                                              | 2018 |     **Adafactor**     | [icml'18](https://arxiv.org/abs/1804.04235)                                                                                                                    | [pytorch](https://github.com/DeadAt0m/adafactor-pytorch)                                                                                                             | GD   |
| Aggregated Momentum: Stability Through Passive Damping                                                                     | 2018 |         AggMo         | [iclr'19](https://arxiv.org/abs/1804.00325)                                                                                                                    | [pytorch, tf](https://github.com/AtheMathmo/AggMo)                                                                                                                   | GD   |
| Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization                                         | 2018 |      Katyusha X       | [icml'18](https://arxiv.org/abs/1802.03866)                                                                                                                    |                                                                                                                                                                      | VR   |
| WNGrad: Learn the Learning Rate in Gradient Descent                                                                        | 2018 |        WNGrad         | [arxiv](https://arxiv.org/abs/1803.02865)                                                                                                                      | [C++](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/wn_grad)                                                                                | GD   |
| VR-SGD: A Simple Stochastic Variance Reduction Method for Machine Learning                                                 | 2018 |        VR-SGD         | [IKDE](https://arxiv.org/abs/1802.09932)                                                                                                                       | [C++](https://github.com/jnhujnhu/VR-SGD)                                                                                                                            | GD   |
| signSGD: Compressed Optimisation for Non-Convex Problems                                                                   | 2018 |      **signSGD**      | [icml'18](https://arxiv.org/abs/1802.04434)                                                                                                                    | [mxnet](https://github.com/jxbz/signSGD)                                                                                                                             | GD   |
| Shampoo: Preconditioned Stochastic Tensor Optimization                                                                     | 2018 |      **Shampoo**      | [icml'18](https://arxiv.org/abs/1802.09568)                                                                                                                    | [tf](https://github.com/Daniil-Selikhanovych/Shampoo_optimizer)                                                                                                      | GD   |
| L4: Practical loss-based stepsize adaptation for deep learning                                                             | 2018 |   L4{Adam,Momentum}   | [neurips'18](https://arxiv.org/abs/1802.05074)                                                                                                                 | [pytorch, tf](https://github.com/martius-lab/l4-optimizer)                                                                                                           | GD   |
| On the Convergence of Adam and Beyond                                                                                      | 2018 |  **AMSGrad**, AdamNC  | [iclr'18](https://arxiv.org/as/1904.09237)                                                                                                                     | [pytorch](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6)                                                   | GD   |
| SW-SGD: The Sliding Window Stochastic Gradient Descent Algorithm                                                           | 2017 |        SW-SGD         | [PCS](https://www.sciencedirect.com/science/article/pii/S1877050917306221)                                                                                     |                                                                                                                                                                      | GD   |
| Improving Generalization Performance by Switching from Adam to SGD                                                         | 2017 |         SWATS         | [iclr'18](https://arxiv.org/abs/1712.07628)                                                                                                                    | [pytorch](https://github.com/Mrpatekful/swats)                                                                                                                       | GD   |
| Noisy Natural Gradient as Variational Inference                                                                            | 2017 |  Noisy {Adam,K-FAC}   | [icml'18](https://arxiv.org/abs/1712.02390)                                                                                                                    | [tf](https://github.com/gd-zhang/noisy-K-FAC)                                                                                                                        | GD   |
| AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training                                    | 2017 |        AdaComp        | [aaai'18](https://arxiv.org/abs/1712.02679)                                                                                                                    |                                                                                                                                                                      | GD   |
| AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks                                                           | 2017 |       AdaBatch        | [iclr-W'18](https://arxiv.org/abs/1712.02029)                                                                                                                  | [PyTorch](https://github.com/GXU-GMU-MICCAI/AdaBatch-numerical-experiments)                                                                                          | GD   |
| First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time                                    | 2017 |         NEON          | [neurips'18](https://arxiv.org/abs/1711.01944)                                                                                                                 |                                                                                                                                                                      | GD   |
| BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning                                                  | 2017 |        BPGrad         | [cvpr'18](https://arxiv.org/abs/1711.06959)                                                                                                                    | [matlab](https://github.com/RyanCV/BPGrad)                                                                                                                           | GD   |
| Decoupled Weight Decay Regularization                                                                                      | 2017 |    **AdamW**,SGDW     | [iclr'19](https://arxiv.org/abs/1711.05101)                                                                                                                    | [lua](https://github.com/loshchil/AdamW-and-SGDW)                                                                                                                    | GD   |
| Evolving Deep Convolutional Neural Networks for Image Classification                                                       | 2017 |        EvoCNN         | [ITEC](https://arxiv.org/abs/1710.10741)                                                                                                                       | [python](https://github.com/MagnusCaligo/EvoCNN)                                                                                                                     | E    |
| Normalized Direction-preserving Adam                                                                                       | 2017 |        ND-Adam        | [arxiv](https://arxiv.org/abs/1709.04546)                                                                                                                      | [pytorch, tf](https://github.com/zj10/ND-Adam)                                                                                                                       | GD   |
| Regularizing and Optimizing LSTM Language Models                                                                           | 2017 |        NT-ASGD        | [iclr'18](https://arxiv.org/abs/1708.02182)                                                                                                                    | [pytorch](https://github.com/salesforce/awd-lstm-lm)                                                                                                                 | GD   |
| Natasha 2: Faster Non-Convex Optimization Than SGD                                                                         | 2017 |    Natasha{1.5,2}     | [neurips'18](https://arxiv.org/abs/1708.08694)                                                                                                                 |                                                                                                                                                                      | GD   |
| Large Batch Training of Convolutional Networks                                                                             | 2017 |       **LARS**        | [arxiv](https://arxiv.org/abs/1708.03888)                                                                                                                      | [pytorch](https://github.com/noahgolmant/pytorch-lars)                                                                                                               | GD   |
| Practical Gauss-Newton Optimisation for Deep Learning                                                                      | 2017 |      KFRA, KFLR       | [icml'17](https://arxiv.org/abs/1706.03662)                                                                                                                    |                                                                                                                                                                      | GD   |
| YellowFin and the Art of Momentum Tuning                                                                                   | 2017 |       YellowFin       | [arxiv](https://arxiv.org/abs/1706.03471)                                                                                                                      | [tf](https://github.com/JianGoForIt/YellowFin)                                                                                                                       | GD   |
| Variants of RMSProp and Adagrad with Logarithmic Regret Bounds                                                             | 2017 | SC-{Adagrad,RMSProp}  | [icml'17](https://arxiv.org/abs/1706.05507)                                                                                                                    | [pytorch](https://github.com/mmahesh/variants-of-rmsprop-and-adagrad)                                                                                                | GD   |
| Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients                                                  | 2017 |        M-SVAG         | [icml'18](https://arxiv.org/abs/1705.07774)                                                                                                                    | [tf](https://github.com/lballes/msvag)                                                                                                                               | GD   |
| Training Deep Networks without Learning Rates Through Coin Betting                                                         | 2017 |         COCOB         | [neurips'17](https://arxiv.org/abs/1705.07795)                                                                                                                 | [tf](https://github.com/bremen79/cocob)                                                                                                                              | GD   |
| Sub-sampled Cubic Regularization for Non-convex Optimization                                                               | 2017 |          SCR          | [icml'17](https://arxiv.org/abs/1705.05933)                                                                                                                    | [numpy](https://github.com/dalab/subsampled_cubic_regularization)                                                                                                    | S    |
| Online Convex Optimization with Unconstrained Domains and Losses                                                           | 2017 |      RescaledExp      | [neurips'16](https://arxiv.org/abs/1703.02622)                                                                                                                 |                                                                                                                                                                      | GD   |
| Evolving Deep Neural Networks                                                                                              | 2017 |      CoDeepNEAT       | [arxiv](https://arxiv.org/abs/1703.00548)                                                                                                                      | [tf](https://github.com/sbcblab/Keras-CoDeepNEAT)                                                                                                                    | E    |
| SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient                                    | 2017 |         SARAH         | [icml'17](https://arxiv.org/abs/1703.00102)                                                                                                                    |                                                                                                                                                                      | VR   |
| IQN: An Incremental Quasi-Newton Method with Local Superlinear Convergence Rate                                            | 2017 |          IQN          | [icassp'17](https://arxiv.org/abs/1702.00709)                                                                                                                  | [C++](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/iqn)                                                                                    | GD,S |
| NMODE --- Neuro-MODule Evolution                                                                                           | 2017 |         NMODE         | [arxiv](https://arxiv.org/abs/1701.05121)                                                                                                                      | [C++](https://github.com/kzahedi/NMODE)                                                                                                                              | E    |
| The Whale Optimization Algorithm                                                                                           | 2016 |          WOA          | [AES](https://www.sciencedirect.com/science/article/abs/pii/S0965997816300163)                                                                                 | [numpy](https://github.com/docwza/woa)                                                                                                                               | E    |
| Incorporating Nesterov Momentum into Adam                                                                                  | 2016 |         Nadam         | [arxiv](https://openreview.net/forum\?id\=OM0jvwB8jIp57ZJjtNEZ)                                                                                                | [pytorch](https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/nadam.py)                                                                         | GD   |
| Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates                                | 2016 |          Eve          | [arxiv](https://arxiv.org/abs/1611.01505)                                                                                                                      | [pytorch](https://github.com/K2OTO/Eve)                                                                                                                              | GD   |
| Direct Feedback Alignment Provides Learning in Deep Neural Networks                                                        | 2016 |          DFA          | [neurips'16](https://arxiv.org/abs/1609.01596)                                                                                                                 | [numpy](https://github.com/metataro/DirectfeedbackAlignment)                                                                                                         | GD   |
| SGDR: Stochastic Gradient Descent with Warm Restarts                                                                       | 2016 |         SGDR          | [iclr'17](https://arxiv.org/abs/1608.03983)                                                                                                                    | [theano](https://github.com/loshchil/SGDR)                                                                                                                           | GD   |
| Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization                                                      | 2016 |    Damp-oBFGS-Inf     | [SIAM](https://arxiv.org/abs/1607.01231)                                                                                                                       | [pytorch](https://github.com/harryliew/SdLBFGS)                                                                                                                      | GD,S |
| A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic Parallel Optimization from Zeroth-Order to First-Order | 2016 |        ZO-SCD         | [neurips'16](https://arxiv.org/abs/1606.00498)                                                                                                                 |                                                                                                                                                                      | GF   |
| Barzilai-Borwein Step Size for Stochastic Gradient Descent                                                                 | 2016 |     {SGD,SVRG}-BB     | [neurips'16](https://arxiv.org/abs/1605.04131)                                                                                                                 | [numpy](https://github.com/tanconghui/Stochastic_BB)                                                                                                                 | GD   |
| Adaptive Learning Rate via Covariance Matrix Based Preconditioning for Deep Neural Networks                                | 2016 |        SDProp         | [ijcai'17](https://arxiv.org/abs/1605.09593)                                                                                                                   |                                                                                                                                                                      | GD   |
| Katyusha: The First Direct Acceleration of Stochastic Gradient Methods                                                     | 2016 |       Katyusha        | [stoc'17](https://arxiv.org/abs/1603.05953)                                                                                                                    |                                                                                                                                                                      | VR   |
| Accelerating SVRG via second-order information                                                                             | 2015 |      SVRG+{I,II}      | [arxiv](https://opt-ml.org/oldopt/opt15/papers.html)                                                                                                           |                                                                                                                                                                      | GD,S |
| adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs                                                                | 2015 |         adaQN         | [ecml'16](https://arxiv.org/abs/1511.01169)                                                                                                                    | [numpy](https://github.com/david-cortes/stochQN)                                                                                                                     | GD,S |
| A Linearly-Convergent Stochastic L-BFGS Algorithm                                                                          | 2015 |       SVRG-SQN        | [aistats](https://arxiv.org/abs/1508.02087)                                                                                                                    | [julia](https://github.com/pcmoritz/slbfgs)                                                                                                                          | GD,S |
| Optimizing Neural Networks with Kronecker-factored Approximate Curvature                                                   | 2015 |       **K-FAC**       | [icml'15](https://arxiv.org/abs/1503.05671)                                                                                                                    | [tf](https://github.com/tensorflow/kfac)                                                                                                                             | GD   |
| Probabilistic Line Searches for Stochastic Optimization                                                                    | 2015 |        ProbLS         | [JMLR](https://arxiv.org/abs/1502.02846)                                                                                                                       |                                                                                                                                                                      | GD   |
| Scale-Free Algorithms for Online Linear Optimization                                                                       | 2015 |        AdaFTRL        | [alt'15](https://arxiv.org/abs/1502.05744)                                                                                                                     |                                                                                                                                                                      | GD   |
| Adam: A Method for Stochastic Optimization                                                                                 | 2014 |   **Adam**, AdaMax    | [iclr'15](https://arxiv.org/abs/1412.6980)                                                                                                                     | [pytorch](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamax.py#L5)                                                 | GD   |
| Random feedback weights support learning in deep neural networks                                                           | 2014 |          FA           | [arxiv](https://arxiv.org/abs/1411.0247)                                                                                                                       | [pytorch](https://github.com/jsalbert/biotorch)                                                                                                                      | GD   |
| A Computationally Efficient Limited Memory CMA-ES for Large Scale Optimization                                             | 2014 |       LM-CMA-ES       | [gecco'14](https://arxiv.org/abs/1404.5520)                                                                                                                    |                                                                                                                                                                      | E    |
| A Proximal Stochastic Gradient Method with Progressive Variance Reduction                                                  | 2014 |       Prox-SVRG       | [SIAM](https://arxiv.org/abs/1403.4699)                                                                                                                        | [tf, numpy](https://github.com/unc-optimization/StochasticProximalMethods)                                                                                           | VR   |
| RES: Regularized Stochastic BFGS Algorithm                                                                                 | 2014 |     Reg-oBFGS-Inf     | [arxiv](https://arxiv.org/abs/1401.7625)                                                                                                                       |                                                                                                                                                                      | GD,S |
| A Stochastic Quasi-Newton Method for Large-Scale Optimization                                                              | 2014 |          SQN          | [SIAM](https://arxiv.org/abs/1401.7020)                                                                                                                        | [matlab](https://github.com/keskarnitish/minSQN)                                                                                                                     | GD,S |
| SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives                         | 2014 |         SAGA          | [neurips'14](https://arxiv.org/abs/1407.0202)                                                                                                                  | [numpy](https://github.com/elmahdichayti/SAGA)                                                                                                                       | VR   |
| Accelerating stochastic gradient descent using predictive variance reduction                                               | 2013 |         SVRG          | [neurips'13](https://papers.nips.cc/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html)                                                            | [pytorch](https://github.com/kilianFatras/variance_reduced_neural_networks)                                                                                          | VR   |
| Ad Click Prediction: a View from the Trenches                                                                              | 2013 |       **FTRL**        | [kdd'13](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf)                                                         | [pytorch](https://github.com/guocheng2018/FTRL-pytorch)                                                                                                              | GD   |
| Semi-Stochastic Gradient Descent Methods                                                                                   | 2013 |         S2GD          | [arxiv](https://arxiv.org/abs/1312.1666)                                                                                                                       |                                                                                                                                                                      | VR   |
| Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming                                            | 2013 |        ZO-SGD         | [SIAM](https://arxiv.org/abs/1309.5549)                                                                                                                        |                                                                                                                                                                      | GF   |
| Mini-batch Stochastic Approximation Methods for Nonconvex Stochastic Composite Optimization                                | 2013 |   ZO-{ProxSGD,PSGD}   | [arxiv](https://arxiv.org/abs/1308.6594)                                                                                                                       |                                                                                                                                                                      | GF   |
| Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients                                   | 2013 |        vSGD-fd        | [arxiv](https://arxiv.org/abs/1301.3764)                                                                                                                       |                                                                                                                                                                      | GD   |
| Neural Networks for Machine Learning                                                                                       | 2012 |      **RMSProp**      | [coursera](http://www.cs.toronto.edu/\~hinton/coursera/lecture6/lec6.pdf)                                                                                      | [tf](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/rmsprop.py)                                                                     | GD   |
| An Enhanced Hypercube-Based Encoding for Evolving the Placement, Density, and Connectivity of Neurons                      | 2012 |     ES-HyperNEAT      | [AL](https://ieeexplore.ieee.org/document/6792180)                                                                                                             | [go](https://github.com/yaricom/goESHyperNEAT)                                                                                                                       | E    |
| CMA-TWEANN: efficient optimization of neural networks via self-adaptation and seamless augmentation                        | 2012 |      CMA-TWEANN       | [gecoo'12](https://dl.acm.org/doi/abs/10.1145/2330163.2330288)                                                                                                 |                                                                                                                                                                      | E    |
| ADADELTA: An Adaptive Learning Rate Method                                                                                 | 2012 |     **ADADELTA**      | [arxiv](https://arxiv.org/abs/1212.5701v1)                                                                                                                     | [pytorch](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adadelta.py\#L6)                                              | GD   |
| No More Pesky Learning Rates                                                                                               | 2012 |     vSGD-{b,g,l}      | [icml'13](https://arxiv.org/abs/1206.1106)                                                                                                                     | [lua](https://github.com/rlowrance/vsgd)                                                                                                                             | VR   |
| A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets                                 | 2012 |          SAG          | [neurips'12](https://arxiv.org/abs/1202.6258)                                                                                                                  |                                                                                                                                                                      | VR   |
| CMA-ES: evolution strategies and covariance matrix adaptation                                                              | 2011 |        CMA-ES         | [gecco'12](https://dl.acm.org/doi/10.1145/2001858.2002123)                                                                                                     | [tf](https://github.com/srom/cma-es)                                                                                                                                 | E    |
| Adaptive Subgradient Methods for Online Learning and Stochastic Optimization                                               | 2011 |      **AdaGrad**      | [JMLR](https://jmlr.org/papers/v12/duchi11a.html)                                                                                                              | [pytorch](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html),[C++](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/ada_grad) | GD   |
| AdaDiff: Adaptive Gradient Descent with the Differential of Gradient                                                       | 2010 |        AdaDiff        | [iopscience](https://iopscience.iop.org/article/10.1088/1742-6596/2010/1/012027)                                                                               |                                                                                                                                                                      | GD   |
| A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks                                                        | 2009 |       HyperNEAT       | [AL](https://ieeexplore.ieee.org/document/6792316)                                                                                                             |                                                                                                                                                                      | E    |
| Scalable training of L1-regularized log-linear models                                                                      | 2007 |        OWL-QN         | [acm](https://dl.acm.org/doi/10.1145/1273496.1273501)                                                                                                          | [javascript](https://github.com/langholz/owlqn)                                                                                                                      | GD,S |
| A Stochastic Quasi-Newton Method for Online Convex Optimization                                                            | 2007 |        O-LBFGS        | [icml'07](https://www.researchgate.net/publication/220319999_A_Stochastic_Quasi-Newton_Method_for_Online_Convex_Optimization)                                  |                                                                                                                                                                      | GD,S |
| Online convex programming and generalized infinitesimal gradient ascent                                                    | 2003 |          OGD          | [icml'03](https://dl.acm.org/doi/10.5555/3041838.3041955)                                                                                                      |                                                                                                                                                                      | GD   |
| A Limited Memory Algorithm for Bound Constrained Optimization                                                              | 2003 |       L-BFGS-B        | [SIAM](https://www.researchgate.net/publication/2837734_A_Limited_Memory_Algorithm_for_Bound_Constrained_Optimization)                                         | [fortran, matlab](https://github.com/jonathanschilling/L-BFGS-B)                                                                                                     | GD,S |
| Evolving Neural Networks through Augmenting Topologies                                                                     | 2002 |         NEAT          | [EC](https://ieeexplore.ieee.org/document/6790655)                                                                                                             | [numpy](https://github.com/goktug97/NEAT)                                                                                                                            | E    |
| Trust region methods                                                                                                       | 2000 |    Sub-sampled TR     | [SIAM](https://epubs.siam.org/doi/book/10.1137/1.9780898719857)                                                                                                |                                                                                                                                                                      | S    |
| A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm                                          | 1993 |         RPROP         | [icnn'93](https://ieeexplore.ieee.org/document/298623)                                                                                                         | [pytorch](https://pytorch.org/docs/stable/generated/torch.optim.Rprop.html#torch.optim.Rprop)                                                                        | GD   |
| Acceleration of Stochastic Approximation by Averaging                                                                      | 1992 |         ASGD          | [SIAM](https://epubs.siam.org/doi/abs/10.1137/0330046?journalCode=sjcodc)                                                                                      | [pytorch](https://pytorch.org/docs/stable/generated/torch.optim.ASGD.html)                                                                                           | GD   |
| Particle swarm optimization                                                                                                | 1995 |          PSO          | [icnn'95](https://ieeexplore.ieee.org/document/488968)                                                                                                         |                                                                                                                                                                      | E    |
| On the limited memory BFGS method for large scale optimization                                                             | 1989 |      **L-BFGS**       | [MP](https://link.springer.com/article/10.1007/BF01589116)                                                                                                     |                                                                                                                                                                      | GD,S |
| Large-scale linearly constrained optimization                                                                              | 1978 |         MINOS         | [MP](https://link.springer.com/article/10.1007/BF01588950)                                                                                                     | [pytorch](https://pytorch.org/docs/stable/generated/torch.optim.LBFGS.html)                                                                                          | GD,S |
| Some methods of speeding up the convergence of iteration methods                                                           | 1964 | **Polyak (momentum)** | [paper](https://www.sciencedirect.com/science/article/abs/pii/0041555364901375)                                                                                |                                                                                                                                                                      | GD   |
| A Stochastic Approximation Method                                                                                          | 1951 |        **SGD**        | [paper](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full) | [pytorch](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)                                                                                            | GD   |
