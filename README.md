# Awesome-Optimizer

A collection of optimizer-related papers and code.

For the last column, we let `GD` for Gradient Descent, `S` for second-order (quasi-newton) methods, `E` for evolutionary, `GF` for gradient free, `VR` for variance reduced.

| Title                                                                                                                      | Year | Optimizer                                  | Published                                                                                                                                                              | Code                                                                                                                 |       |
| -------------------------------------------------------------------------------------------------------------------------- | ---- | ------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----- |
| A Stochastic Approximation Method                                                                                          | 1951 | SGD                                        | [projecteuclid](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full) | [code](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/sgd)                                   | GD    |
| Some methods of speeding up the convergence of iteration methods                                                           | 1964 | Polyak                                     | [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/0041555364901375)                                                                                |                                                                                                                      | GD    |
| Large-scale linearly constrained optimization                                                                              | 1978 | MINOS                                      | [springerlink](https://link.springer.com/article/10.1007/BF01588950)                                                                                                   |                                                                                                                      | GD, S |
| On the limited memory BFGS method for large scale optimization                                                             | 1989 | L-BFGS                                     | [springerlink](https://link.springer.com/article/10.1007/BF01589116)                                                                                                   |                                                                                                                      | GD, S |
| Particle swarm optimization                                                                                                | 1995 | PSO                                        | [ieee](https://ieeexplore.ieee.org/document/488968)                                                                                                                    |                                                                                                                      | E     |
| Trust region methods                                                                                                       | 2000 | Sub-sampled TR                             | [siam](https://epubs.siam.org/doi/book/10.1137/1.9780898719857)                                                                                                        |                                                                                                                      | S     |
| Evolving Neural Networks through Augmenting Topologies                                                                     | 2002 | NEAT                                       | [ieee](https://ieeexplore.ieee.org/document/6790655)                                                                                                                   | [code](https://github.com/goktug97/NEAT)                                                                             | E     |
| A Limited Memory Algorithm for Bound Constrained Optimization                                                              | 2003 | L-BFGS-B                                   | [researchgate](https://www.researchgate.net/publication/2837734_A_Limited_Memory_Algorithm_for_Bound_Constrained_Optimization)                                         | [code](http://users.iems.northwestern.edu/\~nocedal/lbfgsb.html)                                                     | GD, S |
| Online convex programming and generalized infinitesimal gradient ascent                                                    | 2003 | OGD                                        | [acm](https://dl.acm.org/doi/10.5555/3041838.3041955)                                                                                                                  |                                                                                                                      | GD    |
| A Stochastic Quasi-Newton Method for Online Convex Optimization                                                            | 2007 | O-LBFGS                                    | [researchgate](https://www.researchgate.net/publication/220319999_A_Stochastic_Quasi-Newton_Method_for_Online_Convex_Optimization)                                     |                                                                                                                      | GD, S |
| Scalable training of L1-regularized log-linear models                                                                      | 2007 | OWL-QN                                     | [acm](https://dl.acm.org/doi/10.1145/1273496.1273501)                                                                                                                  | [code](https://github.com/langholz/owlqn)                                                                            | GD, S |
| A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks                                                        | 2009 | HyperNEAT                                  | [ieee](https://ieeexplore.ieee.org/document/6792316)                                                                                                                   |                                                                                                                      | E     |
| AdaDiff: Adaptive Gradient Descent with the Differential of Gradient                                                       | 2010 | AdaDiff                                    | [iopscience](https://iopscience.iop.org/article/10.1088/1742-6596/2010/1/012027)                                                                                       |                                                                                                                      | GD    |
| Adaptive Subgradient Methods for Online Learning and Stochastic Optimization                                               | 2011 | AdaGrad                                    | [jmlr](https://jmlr.org/papers/v12/duchi11a.html)                                                                                                                      | [code](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/ada_grad)                              | GD    |
| CMA-ES: evolution strategies and covariance matrix adaptation                                                              | 2011 | CMA-ES                                     | [acm](https://dl.acm.org/doi/10.1145/2001858.2002123)                                                                                                                  | [code](https://github.com/srom/cma-es)                                                                               | E     |
| ADADELTA: An Adaptive Learning Rate Method                                                                                 | 2012 | ADADELTA                                   | [arxiv](https://arxiv.org/abs/1212.5701v1)                                                                                                                             | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adadelta.py\#L6) | GD    |
| A Stochastic Gradient Method with an Exponential Convergence Rate for Finite Training Sets                                 | 2012 | SAG                                        | [arxiv](https://arxiv.org/abs/1202.6258)                                                                                                                               |                                                                                                                      | VR    |
| An Enhanced Hypercube-Based Encoding for Evolving the Placement, Density, and Connectivity of Neurons                      | 2012 | ES-HyperNEAT                               | [ieee](https://ieeexplore.ieee.org/document/6792180)                                                                                                                   | [code](https://github.com/yaricom/goESHyperNEAT)                                                                     | E     |
| CMA-TWEANN: efficient optimization of neural networks via self-adaptation and seamless augmentation                        | 2012 | CMA-TWEANN                                 | [acm](https://dl.acm.org/doi/abs/10.1145/2330163.2330288)                                                                                                              |                                                                                                                      | E     |
| Neural Networks for Machine Learning                                                                                       | 2012 | RMSProp                                    | [coursera](http://www.cs.toronto.edu/\~hinton/coursera/lecture6/lec6.pdf)                                                                                              | [code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/rmsprop.py)                   | GD    |
| No More Pesky Learning Rates                                                                                               | 2012 | vSGD-b                                     | [arxiv](https://arxiv.org/abs/1206.1106)                                                                                                                               | [code](https://github.com/rlowrance/vsgd)                                                                            | VR    |
| No More Pesky Learning Rates                                                                                               | 2012 | vSGD-g                                     | [arxiv](https://arxiv.org/abs/1206.1106)                                                                                                                               | [code](https://github.com/rlowrance/vsgd)                                                                            | VR    |
| No More Pesky Learning Rates                                                                                               | 2012 | vSGD-l                                     | [arxiv](https://arxiv.org/abs/1206.1106)                                                                                                                               | [code](https://github.com/rlowrance/vsgd)                                                                            | VR    |
| Ad Click Prediction: a View from the Trenches                                                                              | 2013 | FTRL                                       | [google](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41159.pdf)                                                                 |                                                                                                                      | GD    |
| Accelerating stochastic gradient descent using predictive variance reduction                                               | 2013 | SVRG                                       | [neurips](https://papers.nips.cc/paper/2013/hash/ac1dd209cbcc5e5d1c6e28598e8cbbe8-Abstract.html)                                                                       | [code](https://github.com/kilianFatras/variance_reduced_neural_networks)                                             | VR    |
| Adaptive learning rates and parallelization for stochastic, sparse, non-smooth gradients                                   | 2013 | vSGD-fd                                    | [arxiv](https://arxiv.org/abs/1301.3764)                                                                                                                               |                                                                                                                      | GD    |
| Stochastic First- and Zeroth-order Methods for Nonconvex Stochastic Programming                                            | 2013 | ZO-SGD                                     | [arxiv](https://arxiv.org/abs/1309.5549)                                                                                                                               |                                                                                                                      | GF    |
| Mini-batch Stochastic Approximation Methods for Nonconvex Stochastic Composite Optimization                                | 2013 | ZO-ProxSGD                                 | [arxiv](https://arxiv.org/abs/1308.6594)                                                                                                                               |                                                                                                                      | GF    |
| Mini-batch Stochastic Approximation Methods for Nonconvex Stochastic Composite Optimization                                | 2013 | ZO-PSGD                                    | [arxiv](https://arxiv.org/abs/1308.6594)                                                                                                                               |                                                                                                                      | GF    |
| Semi-Stochastic Gradient Descent Methods                                                                                   | 2013 | S2GD                                       | [arxiv](https://arxiv.org/abs/1312.1666)                                                                                                                               |                                                                                                                      | VR    |
| Adam: A Method for Stochastic Optimization                                                                                 | 2014 | Adam                                       | [arxiv](https://arxiv.org/abs/1412.6980)                                                                                                                               | [code](https://paperswithcode.com/paper/adam-a-method-for-stochastic-optimization)                                   | GD    |
| SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives                         | 2014 | SAGA                                       | [arxiv](https://arxiv.org/abs/1407.0202)                                                                                                                               | [code](https://github.com/elmahdichayti/SAGA)                                                                        | VR    |
| A Stochastic Quasi-Newton Method for Large-Scale Optimization                                                              | 2014 | SQN                                        | [arxiv](https://arxiv.org/abs/1401.7020)                                                                                                                               | [code](https://github.com/keskarnitish/minSQN)                                                                       | GD, S |
| RES: Regularized Stochastic BFGS Algorithm                                                                                 | 2014 | Reg-oBFGS-Inf                              | [arxiv](https://arxiv.org/abs/1401.7625)                                                                                                                               |                                                                                                                      | GD, S |
| A Proximal Stochastic Gradient Method with Progressive Variance Reduction                                                  | 2014 | Prox-SVRG                                  | [arxiv](https://arxiv.org/abs/1403.4699)                                                                                                                               | [code](https://github.com/unc-optimization/StochasticProximalMethods)                                                | VR    |
| A Computationally Efficient Limited Memory CMA-ES for Large Scale Optimization                                             | 2014 | LM-CMA-ES                                  | [arxiv](https://arxiv.org/abs/1404.5520)                                                                                                                               |                                                                                                                      | E     |
| Random feedback weights support learning in deep neural networks                                                           | 2014 | FA                                         | [arxiv](https://arxiv.org/abs/1411.0247)                                                                                                                               | [code](https://github.com/jsalbert/biotorch)                                                                         | GD    |
| Adam: A Method for Stochastic Optimization                                                                                 | 2015 | AdaMax                                     | [arxiv](https://arxiv.org/abs/1412.6980)                                                                                                                               | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adamax.py#L5)    | GD    |
| Scale-Free Algorithms for Online Linear Optimization                                                                       | 2015 | AdaFTRL                                    | [arxiv](https://arxiv.org/abs/1502.05744)                                                                                                                              |                                                                                                                      | GD    |
| A Linearly-Convergent Stochastic L-BFGS Algorithm                                                                          | 2015 | SVRG-SQN                                   | [arxiv](https://arxiv.org/abs/1508.02087)                                                                                                                              | [code](https://github.com/pcmoritz/slbfgs)                                                                           | GD, S |
| Accelerating SVRG via second-order information                                                                             | 2015 | SVRG+II: LBFGS                             | [opt](https://opt-ml.org/oldopt/opt15/papers.html)                                                                                                                     |                                                                                                                      | GD, S |
| Accelerating SVRG via second-order information                                                                             | 2015 | SVRG+I: Subsampled Hessian followed by SVT | [opt](https://opt-ml.org/oldopt/opt15/papers.html)                                                                                                                     |                                                                                                                      | GD, S |
| Probabilistic Line Searches for Stochastic Optimization                                                                    | 2015 | ProbLS                                     | [arxiv](https://arxiv.org/abs/1502.02846)                                                                                                                              |                                                                                                                      | GD    |
| Optimizing Neural Networks with Kronecker-factored Approximate Curvature                                                   | 2015 | K-FAC                                      | [arxiv](https://arxiv.org/abs/1503.05671)                                                                                                                              | [code](https://github.com/tensorflow/kfac)                                                                           | GD    |
| adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs                                                                | 2015 | adaQN                                      | [arxiv](https://arxiv.org/abs/1511.01169)                                                                                                                              | [code](https://github.com/david-cortes/stochQN)                                                                      | GD, S |
| Stochastic Quasi-Newton Methods for Nonconvex Stochastic Optimization                                                      | 2016 | Damp-oBFGS-Inf                             | [arxiv](https://arxiv.org/abs/1607.01231)                                                                                                                              | [code](https://github.com/harryliew/SdLBFGS)                                                                         | GD, S |
| Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates                                | 2016 | Eve                                        | [arxiv](https://arxiv.org/abs/1611.01505)                                                                                                                              | [code](https://github.com/K2OTO/Eve)                                                                                 | GD    |
| Incorporating Nesterov Momentum into Adam                                                                                  | 2016 | Nadam                                      | [openreview](https://openreview.net/forum\?id\=OM0jvwB8jIp57ZJjtNEZ)                                                                                                   | [code](https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/nadam.py)                            | GD    |
| The Whale Optimization Algorithm                                                                                           | 2016 | WOA                                        | [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S0965997816300163)                                                                               | [code](https://github.com/docwza/woa)                                                                                | E     |
| Adaptive Learning Rate via Covariance Matrix Based Preconditioning for Deep Neural Networks                                | 2016 | SDProp                                     | [arxiv](https://arxiv.org/abs/1605.09593)                                                                                                                              |                                                                                                                      | GD    |
| Barzilai-Borwein Step Size for Stochastic Gradient Descent                                                                 | 2016 | SGD-BB                                     | [arxiv](https://arxiv.org/abs/1605.04131)                                                                                                                              | [code](https://github.com/tanconghui/Stochastic_BB)                                                                  | GD    |
| Barzilai-Borwein Step Size for Stochastic Gradient Descent                                                                 | 2016 | SVRG-BB                                    | [arxiv](https://arxiv.org/abs/1605.04131)                                                                                                                              | [code](https://github.com/tanconghui/Stochastic_BB)                                                                  | VR    |
| SGDR: Stochastic Gradient Descent with Warm Restarts                                                                       | 2016 | SGDR                                       | [arxiv](https://arxiv.org/abs/1608.03983)                                                                                                                              | [code](https://github.com/loshchil/SGDR)                                                                             | GD    |
| Katyusha: The First Direct Acceleration of Stochastic Gradient Methods                                                     | 2016 | Katyusha                                   | [arxiv](https://arxiv.org/abs/1603.05953)                                                                                                                              |                                                                                                                      | VR    |
| A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic Parallel Optimization from Zeroth-Order to First-Order | 2016 | ZO-SCD                                     | [arxiv](https://arxiv.org/abs/1606.00498)                                                                                                                              |                                                                                                                      | GF    |
| Direct Feedback Alignment Provides Learning in Deep Neural Networks                                                        | 2016 | DFA                                        | [arxiv](https://arxiv.org/abs/1609.01596)                                                                                                                              | [code](https://github.com/metataro/DirectFeedbackAlignment)                                                          | GD    |
| AdaBatch: Adaptive Batch Sizes for Training Deep Neural Networks                                                           | 2017 | AdaBatch                                   | [arxiv](https://arxiv.org/abs/1712.02029)                                                                                                                              | [code](https://github.com/GXU-GMU-MICCAI/AdaBatch-numerical-experiments)                                             | GD    |
| AdaComp : Adaptive Residual Gradient Compression for Data-Parallel Distributed Training                                    | 2017 | AdaComp                                    | [arxiv](https://arxiv.org/abs/1712.02679)                                                                                                                              |                                                                                                                      | GD    |
| SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient                                    | 2017 | SARAH                                      | [arxiv](https://arxiv.org/abs/1703.00102)                                                                                                                              |                                                                                                                      | VR    |
| Sub-sampled Cubic Regularization for Non-convex Optimization                                                               | 2017 | SCR                                        | [arxiv](https://arxiv.org/abs/1705.05933)                                                                                                                              | [code](https://github.com/dalab/subsampled_cubic_regularization)                                                     | S     |
| IQN: An Incremental Quasi-Newton Method with Local Superlinear Convergence Rate                                            | 2017 | IQN                                        | [arxiv](https://arxiv.org/abs/1702.00709)                                                                                                                              | [code](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/iqn)                                   | GD, S |
| Decoupled Weight Decay Regularization                                                                                      | 2017 | AdamW                                      | [arxiv](https://arxiv.org/abs/1711.05101)                                                                                                                              | [code](https://github.com/loshchil/AdamW-and-SGDW)                                                                   | GD    |
| Decoupled Weight Decay Regularization                                                                                      | 2017 | SGDW                                       | [arxiv](https://arxiv.org/abs/1711.05101)                                                                                                                              | [code](https://github.com/loshchil/AdamW-and-SGDW)                                                                   | GD    |
| BPGrad: Towards Global Optimality in Deep Learning via Branch and Pruning                                                  | 2017 | BPGrad                                     | [arxiv](https://arxiv.org/abs/1711.06959)                                                                                                                              | [code](https://github.com/RyanCV/BPGrad)                                                                             | GD    |
| Training Deep Networks without Learning Rates Through Coin Betting                                                         | 2017 | COCOB                                      | [arxiv](https://arxiv.org/abs/1705.07795)                                                                                                                              | [code](https://github.com/bremen79/cocob)                                                                            | GD    |
| Practical Gauss-Newton Optimisation for Deep Learning                                                                      | 2017 | KFLR                                       | [arxiv](https://arxiv.org/abs/1706.03662)                                                                                                                              |                                                                                                                      | GD    |
| Practical Gauss-Newton Optimisation for Deep Learning                                                                      | 2017 | KFRA                                       | [arxiv](https://arxiv.org/abs/1706.03662)                                                                                                                              |                                                                                                                      | GD    |
| Large Batch Training of Convolutional Networks                                                                             | 2017 | LARS                                       | [arxiv](https://arxiv.org/abs/1708.03888)                                                                                                                              | [code](https://github.com/noahgolmant/pytorch-lars)                                                                  | GD    |
| Dissecting Adam: The Sign, Magnitude and Variance of Stochastic Gradients                                                  | 2017 | M-SVAG                                     | [arxiv](https://arxiv.org/abs/1705.07774)                                                                                                                              | [code](https://github.com/lballes/msvag)                                                                             | GD    |
| Normalized Direction-preserving Adam                                                                                       | 2017 | ND-Adam                                    | [arxiv](https://arxiv.org/abs/1709.04546)                                                                                                                              | [code](https://github.com/zj10/ND-Adam)                                                                              | GD    |
| Noisy Natural Gradient as Variational Inference                                                                            | 2017 | Noisy Adam                                 | [arxiv](https://arxiv.org/abs/1712.02390)                                                                                                                              | [code](https://github.com/gd-zhang/noisy-K-FAC)                                                                      | GD    |
| Noisy Natural Gradient as Variational Inference                                                                            | 2017 | Noisy K-FAC                                | [arxiv](https://arxiv.org/abs/1712.02390)                                                                                                                              | [code](https://github.com/gd-zhang/noisy-K-FAC)                                                                      | GD    |
| Evolving Deep Neural Networks                                                                                              | 2017 | CoDeepNEAT                                 | [arxiv](https://arxiv.org/abs/1703.00548)                                                                                                                              | [code](https://github.com/sbcblab/Keras-CoDeepNEAT)                                                                  | E     |
| Evolving Deep Convolutional Neural Networks for Image Classification                                                       | 2017 | EvoCNN                                     | [arxiv](https://arxiv.org/abs/1710.10741)                                                                                                                              | [code](https://github.com/MagnusCaligo/EvoCNN)                                                                       | E     |
| NMODE --- Neuro-MODule Evolution                                                                                           | 2017 | NMODE                                      | [arxiv](https://arxiv.org/abs/1701.05121)                                                                                                                              | [code](https://github.com/kzahedi/NMODE)                                                                             | E     |
| Online Convex Optimization with Unconstrained Domains and Losses                                                           | 2017 | RescaledExp                                | [arxiv](https://arxiv.org/abs/1703.02622)                                                                                                                              |                                                                                                                      | GD    |
| Variants of RMSProp and Adagrad with Logarithmic Regret Bounds                                                             | 2017 | SC-Adagrad                                 | [arxiv](https://arxiv.org/abs/1706.05507)                                                                                                                              | [code](https://github.com/mmahesh/variants-of-rmsprop-and-adagrad)                                                   | GD    |
| Variants of RMSProp and Adagrad with Logarithmic Regret Bounds                                                             | 2017 | SC-RMSProp                                 | [arxiv](https://arxiv.org/abs/1706.05507)                                                                                                                              | [code](https://github.com/mmahesh/variants-of-rmsprop-and-adagrad)                                                   | GD    |
| Improving Generalization Performance by Switching from Adam to SGD                                                         | 2017 | SWATS                                      | [arxiv](https://arxiv.org/abs/1712.07628)                                                                                                                              | [code](https://github.com/Mrpatekful/swats)                                                                          | GD    |
| YellowFin and the Art of Momentum Tuning                                                                                   | 2017 | YellowFin                                  | [arxiv](https://arxiv.org/abs/1706.03471)                                                                                                                              | [code](https://github.com/JianGoForIt/YellowFin)                                                                     | GD    |
| Natasha 2: Faster Non-Convex Optimization Than SGD                                                                         | 2017 | Natasha2                                   | [arxiv](https://arxiv.org/abs/1708.08694)                                                                                                                              |                                                                                                                      | GD    |
| Natasha 2: Faster Non-Convex Optimization Than SGD                                                                         | 2017 | Natasha1.5                                 | [arxiv](https://arxiv.org/abs/1708.08694)                                                                                                                              |                                                                                                                      | GD    |
| Regularizing and Optimizing LSTM Language Models                                                                           | 2017 | NT-ASGD                                    | [arxiv](https://arxiv.org/abs/1708.02182)                                                                                                                              | [code](https://github.com/salesforce/awd-lstm-lm)                                                                    | GD    |
| SW-SGD: The Sliding Window Stochastic Gradient Descent Algorithm                                                           | 2017 | SW-SGD                                     | [sciencedirect](https://www.sciencedirect.com/science/article/pii/S1877050917306221)                                                                                   |                                                                                                                      | GD    |
| Adafactor: Adaptive Learning Rates with Sublinear Memory Cost                                                              | 2018 | Adafactor                                  | [arxiv](https://arxiv.org/abs/1804.04235)                                                                                                                              | [code](https://github.com/DeadAt0m/adafactor-pytorch)                                                                | GD    |
| Quasi-hyperbolic momentum and Adam for deep learning                                                                       | 2018 | QHAdam                                     | [arxiv](https://arxiv.org/abs/1810.06801)                                                                                                                              | [code](https://github.com/facebookresearch/qhoptim)                                                                  | GD    |
| Online Adaptive Methods, Universality and Acceleration                                                                     | 2018 | AcceleGrad                                 | [arxiv](https://arxiv.org/abs/1809.02864)                                                                                                                              |                                                                                                                      | GD    |
| Bayesian filtering unifies adaptive and non-adaptive neural network optimization methods                                   | 2018 | AdaBayes                                   | [arxiv](https://arxiv.org/abs/1807.07540)                                                                                                                              | [code](https://github.com/LaurenceA/adabayes)                                                                        | GD    |
| On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization                                          | 2018 | AdaFom                                     | [arxiv](https://arxiv.org/abs/1808.02941)                                                                                                                              |                                                                                                                      | GD    |
| Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis                                               | 2018 | EKFAC                                      | [arxiv](https://arxiv.org/abs/1806.03884)                                                                                                                              | [code](https://github.com/Thrandis/EKFAC-pytorch)                                                                    | GD    |
| AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods                                                  | 2018 | AdaShift                                   | [arxiv](https://arxiv.org/abs/1810.00143)                                                                                                                              | [code](https://github.com/MichaelKonobeev/adashift)                                                                  | GD    |
| Practical Bayesian Learning of Neural Networks via Adaptive Optimisation Methods                                           | 2018 | BADAM                                      | [arxiv](https://arxiv.org/abs/1811.03679)                                                                                                                              | [code](https://github.com/skezle/BADAM)                                                                              | GD    |
| Small steps and giant leaps: Minimal Newton solvers for Deep Learning                                                      | 2018 | Curveball                                  | [arxiv](https://arxiv.org/abs/1805.08095)                                                                                                                              | [code](https://github.com/jotaf98/curveball)                                                                         | GD    |
| GADAM: Genetic-Evolutionary ADAM for Deep Neural Network Optimization                                                      | 2018 | GADAM                                      | [arxiv](https://arxiv.org/abs/1805.07500)                                                                                                                              |                                                                                                                      | GD    |
| HyperAdam: A Learnable Task-Adaptive Adam for Network Training                                                             | 2018 | HyperAdam                                  | [arxiv](https://arxiv.org/abs/1811.08996)                                                                                                                              | [code](https://github.com/ShipengWang/HyperAdam)                                                                     | GD    |
| L4: Practical loss-based stepsize adaptation for deep learning                                                             | 2018 | L4Adam                                     | [arxiv](https://arxiv.org/abs/1802.05074)                                                                                                                              | [code](https://github.com/martius-lab/l4-optimizer)                                                                  | GD    |
| L4: Practical loss-based stepsize adaptation for deep learning                                                             | 2018 | L4Momentum                                 | [arxiv](https://arxiv.org/abs/1802.05074)                                                                                                                              | [code](https://github.com/martius-lab/l4-optimizer)                                                                  | GD    |
| Nostalgic Adam: Weighting more of the past gradients when designing the adaptive learning rate                             | 2018 | NosAdam                                    | [arxiv](https://arxiv.org/abs/1805.07557)                                                                                                                              | [code](https://github.com/andrehuang/NostalgicAdam-NosAdam)                                                          | GD    |
| Closing the Generalization Gap of Adaptive Gradient Methods in Training Deep Neural Networks                               | 2018 | Padam                                      | [arxiv](https://arxiv.org/abs/1806.06763)                                                                                                                              | [code](https://github.com/uclaml/Padam)                                                                              | GD    |
| Quasi-hyperbolic momentum and Adam for deep learning                                                                       | 2018 | QHM                                        | [arxiv](https://arxiv.org/abs/1810.06801)                                                                                                                              | [code](https://github.com/facebookresearch/qhoptim)                                                                  | GD    |
| Optimal Adaptive and Accelerated Stochastic Gradient Descent                                                               | 2018 | A2GradExp                                  | [arxiv](https://arxiv.org/abs/1810.00553)                                                                                                                              | [code](https://github.com/severilov/A2Grad_optimizer)                                                                | GD    |
| Optimal Adaptive and Accelerated Stochastic Gradient Descent                                                               | 2018 | A2GradInc                                  | [arxiv](https://arxiv.org/abs/1810.00553)                                                                                                                              | [code](https://github.com/severilov/A2Grad_optimizer)                                                                | GD    |
| Optimal Adaptive and Accelerated Stochastic Gradient Descent                                                               | 2018 | A2GradUni                                  | [arxiv](https://arxiv.org/abs/1810.00553)                                                                                                                              | [code](https://github.com/severilov/A2Grad_optimizer)                                                                | GD    |
| Shampoo: Preconditioned Stochastic Tensor Optimization                                                                     | 2018 | Shampoo                                    | [arxiv](https://arxiv.org/abs/1802.09568)                                                                                                                              | [code](https://github.com/Daniil-Selikhanovych/Shampoo_optimizer)                                                    | GD    |
| signSGD: Compressed Optimisation for Non-Convex Problems                                                                   | 2018 | signSGD                                    | [arxiv](https://arxiv.org/abs/1802.04434)                                                                                                                              | [code](https://github.com/jxbz/signSGD)                                                                              | GD    |
| Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam                                                    | 2018 | VAdam                                      | [arxiv](https://arxiv.org/abs/1806.04854)                                                                                                                              | [code](https://github.com/emtiyaz/vadam)                                                                             | GD    |
| VR-SGD: A Simple Stochastic Variance Reduction Method for Machine Learning                                                 | 2018 | VR-SGD                                     | [arxiv](https://arxiv.org/abs/1802.09932)                                                                                                                              | [code](https://github.com/jnhujnhu/VR-SGD)                                                                           | GD    |
| WNGrad: Learn the Learning Rate in Gradient Descent                                                                        | 2018 | WNGrad                                     | [arxiv](https://arxiv.org/abs/1803.02865)                                                                                                                              | [code](https://github.com/mlpack/ensmallen/tree/master/include/ensmallen_bits/wn_grad)                               | GD    |
| Adaptive Methods for Nonconvex Optimization                                                                                | 2018 | Yogi                                       | [neurips](https://papers.nips.cc/paper/2018/hash/90365351ccc7437a1309dc64e4db32a3-Abstract.html)                                                                       | [code](https://github.com/4rtemi5/Yogi-Optimizer_Keras)                                                              | GD    |
| First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time                                    | 2018 | NEON                                       | [arxiv](https://arxiv.org/abs/1711.01944)                                                                                                                              |                                                                                                                      | GD    |
| Katyusha X: Practical Momentum Method for Stochastic Sum-of-Nonconvex Optimization                                         | 2018 | Katyusha X                                 | [arxiv](https://arxiv.org/abs/1802.03866)                                                                                                                              |                                                                                                                      | VR    |
| PSA-CMA-ES: CMA-ES with population size adaptation                                                                         | 2018 | PSA-CMA-ES                                 | [acm](https://dl.acm.org/doi/10.1145/3205455.3205467)                                                                                                                  |                                                                                                                      | E     |
| AdaGrad Stepsizes: Sharp Convergence Over Nonconvex Landscapes                                                             | 2018 | AdaGrad-Norm                               | [arxiv](https://arxiv.org/abs/1806.01811)                                                                                                                              | [code](https://github.com/xwuShirley/pytorch/blob/master/torch/optim/adagradnorm.py)                                 | GD    |
| Aggregated Momentum: Stability Through Passive Damping                                                                     | 2018 | AggMo                                      | [arxiv](https://arxiv.org/abs/1804.00325)                                                                                                                              | [code](https://github.com/AtheMathmo/AggMo)                                                                          | GD    |
| Accelerating SGD with momentum for over-parameterized learning                                                             | 2018 | MaSS                                       | [arxiv](https://arxiv.org/abs/1810.13395)                                                                                                                              | [code](https://github.com/ts66395/MaSS)                                                                              | GD    |
| SADAGRAD: Strongly Adaptive Stochastic Gradient Methods                                                                    | 2018 | SADAGRAD                                   | [mlr](http://proceedings.mlr.press/v80/chen18m.html)                                                                                                                   |                                                                                                                      | GD    |
| Deep Frank-Wolfe For Neural Network Optimization                                                                           | 2018 | DFW                                        | [arxiv](https://arxiv.org/abs/1811.07591)                                                                                                                              | [code](https://github.com/oval-group/dfw)                                                                            | GD    |
| On the Convergence of AdaGrad with Momentum for Training Deep Neural Networks                                              | 2018 | AdaHB                                      | [deepai](https://deepai.org/publication/on-the-convergence-of-adagrad-with-momentum-for-training-deep-neural-networks)                                                 |                                                                                                                      | GD    |
| On the Convergence of AdaGrad with Momentum for Training Deep Neural Networks                                              | 2018 | AdaNAG                                     | [deepai](https://deepai.org/publication/on-the-convergence-of-adagrad-with-momentum-for-training-deep-neural-networks)                                                 |                                                                                                                      | GD    |
| Kalman Gradient Descent: Adaptive Variance Reduction in Stochastic Optimization                                            | 2018 | KGD                                        | [arxiv](https://arxiv.org/abs/1810.12273)                                                                                                                              | [code](https://github.com/jamesvuc/KGD)                                                                              | GD    |
| On the Convergence of Adam and Beyond                                                                                      | 2019 | AMSGrad                                    | [arxiv](https://arxiv.org/abs/1904.09237)                                                                                                                              | [code](https://github.com/pytorch/pytorch/blob/b7bda236d18815052378c88081f64935427d7716/torch/optim/adam.py#L6)      | GD    |
| Local AdaAlter: Communication-Efficient Stochastic Gradient Descent with Adaptive Learning Rates                           | 2019 | AdaAlter                                   | [arxiv](https://arxiv.org/abs/1911.09030)                                                                                                                              | [code](https://github.com/xcgoner/AISTATS2020-AdaAlter-GluonNLP)                                                     | GD    |
| Adaptive Gradient Methods with Dynamic Bound of Learning Rate                                                              | 2019 | AdaBound                                   | [arxiv](https://arxiv.org/abs/1902.09843)                                                                                                                              | [code](https://github.com/Luolc/AdaBound)                                                                            | GD    |
| Does Adam optimizer keep close to the optimal point?                                                                       | 2019 | AdaFix                                     | [arxiv](https://arxiv.org/abs/1911.00289)                                                                                                                              |                                                                                                                      | GD    |
| Adaloss: Adaptive Loss Function for Landmark Localization                                                                  | 2019 | Adaloss                                    | [arxiv](https://arxiv.org/abs/1908.01070)                                                                                                                              |                                                                                                                      | GD    |
| A new perspective in understanding of Adam-Type algorithms and beyond                                                      | 2019 | AdamAL                                     | [openreview](https://openreview.net/forum\?id\=SyxM51BYPB)                                                                                                             | [code](https://www.dropbox.com/s/qgqhg6znuimzci9/adamAL.py\?dl\=0)                                                   | GD    |
| On the Convergence of Adam and Beyond                                                                                      | 2019 | AdamNC                                     | [arxiv](https://arxiv.org/abs/1904.09237)                                                                                                                              |                                                                                                                      | GD    |
| Lookahead Optimizer: k steps forward, 1 step back                                                                          | 2019 | Lookahead                                  | [arxiv](https://arxiv.org/abs/1907.08610)                                                                                                                              | [code](https://github.com/michaelrzhang/lookahead)                                                                   | GD    |
| On Higher-order Moments in Adam                                                                                            | 2019 | HAdam                                      | [arxiv](https://arxiv.org/abs/1910.06878)                                                                                                                              |                                                                                                                      | GD    |
| An Adaptive and Momental Bound Method for Stochastic Learning                                                              | 2019 | AdaMod                                     | [arxiv](https://arxiv.org/abs/1910.12249)                                                                                                                              | [code](https://github.com/lancopku/AdaMod)                                                                           | GD    |
| On the Convergence Proof of AMSGrad and a New Version                                                                      | 2019 | AdamX                                      | [arxiv](https://arxiv.org/abs/1904.03590)                                                                                                                              |                                                                                                                      | GD    |
| Second-order Information in First-order Optimization Methods                                                               | 2019 | AdaSqrt                                    | [arxiv](https://arxiv.org/abs/1912.09926)                                                                                                                              | [code](https://github.com/OSI-Group/AdaSqrt)                                                                         | GD    |
| Adathm: Adaptive Gradient Method Based on Estimates of Third-Order Moments                                                 | 2019 | Adathm                                     | [ieee](https://ieeexplore.ieee.org/document/8923615)                                                                                                                   |                                                                                                                      | GD    |
| Domain-independent Dominance of Adaptive Methods                                                                           | 2019 | Delayed Adam                               | [arxiv](https://arxiv.org/abs/1912.01823)                                                                                                                              | [code](https://github.com/lolemacs/avagrad)                                                                          | GD    |
| Domain-independent Dominance of Adaptive Methods                                                                           | 2019 | AvaGrad                                    | [arxiv](https://arxiv.org/abs/1912.01823)                                                                                                                              | [code](https://github.com/lolemacs/avagrad)                                                                          | GD    |
| Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates                                            | 2019 | ArmijoLS                                   | [arxiv](https://arxiv.org/abs/1905.09997)                                                                                                                              | [code](https://github.com/IssamLaradji/sls)                                                                          | GD    |
| An Adaptive Remote Stochastic Gradient Method for Training Neural Networks                                                 | 2019 | ARSG                                       | [arxiv](https://arxiv.org/abs/1905.01422)                                                                                                                              | [code](https://github.com/rationalspark/NAMSG)                                                                       | GD    |
| BGADAM: Boosting based Genetic-Evolutionary ADAM for Neural Network Optimization                                           | 2019 | BGADAM                                     | [arxiv](https://arxiv.org/abs/1908.08015)                                                                                                                              |                                                                                                                      | GD    |
| CProp: Adaptive Learning Rate Scaling from Past Gradient Conformity                                                        | 2019 | CProp                                      | [arxiv](https://arxiv.org/abs/1912.11493)                                                                                                                              | [code](https://github.com/phizaz/cprop)                                                                              | GD    |
| DADAM: A Consensus-based Distributed Adaptive Gradient Method for Online Optimization                                      | 2019 | DADAM                                      | [arxiv](https://arxiv.org/abs/1901.09109)                                                                                                                              | [code](https://github.com/Tarzanagh/DADAM)                                                                           | GD    |
| diffGrad: An Optimization Method for Convolutional Neural Networks                                                         | 2019 | diffGrad                                   | [arxiv](https://arxiv.org/abs/1909.11015)                                                                                                                              | [code](https://github.com/shivram1987/diffGrad)                                                                      | GD    |
| Gradient-only line searches: An Alternative to Probabilistic Line Searches                                                 | 2019 | GOLS-I                                     | [arxiv](https://arxiv.org/abs/1903.09383)                                                                                                                              |                                                                                                                      | GD    |
| Large Batch Optimization for Deep Learning: Training BERT in 76 minutes                                                    | 2019 | LAMB                                       | [arxiv](https://arxiv.org/abs/1904.00962)                                                                                                                              | [code](https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py)                        | GD    |
| An Adaptive Remote Stochastic Gradient Method for Training Neural Networks                                                 | 2019 | NAMSB                                      | [arxiv](https://arxiv.org/abs/1905.01422)                                                                                                                              | [code](https://github.com/rationalspark/NAMSG)                                                                       | GD    |
| An Adaptive Remote Stochastic Gradient Method for Training Neural Networks                                                 | 2019 | NAMSG                                      | [arxiv](https://arxiv.org/abs/1905.01422)                                                                                                                              | [code](https://github.com/rationalspark/NAMSG)                                                                       | GD    |
| Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks                                 | 2019 | Novograd                                   | [arxiv](https://arxiv.org/abs/1905.11286)                                                                                                                              | [code](https://github.com/convergence-lab/novograd)                                                                  | GD    |
| Fast-DENSER++: Evolving Fully-Trained Deep Artificial Neural Networks                                                      | 2019 | F-DENSER++                                 | [arxiv](https://arxiv.org/abs/1905.02969)                                                                                                                              | [code](https://github.com/fillassuncao/fast-denser)                                                                  | E     |
| Fast DENSER: Efficient Deep NeuroEvolution                                                                                 | 2019 | F-DENSER                                   | [researchgate](https://www.researchgate.net/publication/332306893_Fast_DENSER_Efficient_Deep_NeuroEvolution)                                                           | [code](https://github.com/fillassuncao/fast-denser)                                                                  | E     |
| Parabolic Approximation Line Search for DNNs                                                                               | 2019 | PAL                                        | [arxiv](https://arxiv.org/abs/1903.11991)                                                                                                                              | [code](https://github.com/cogsys-tuebingen/PAL)                                                                      | GD    |
| The Role of Memory in Stochastic Optimization                                                                              | 2019 | PolyAdam                                   | [arxiv](https://arxiv.org/abs/1907.01678)                                                                                                                              |                                                                                                                      | GD    |
| PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization                                             | 2019 | PowerSGD                                   | [arxiv](https://arxiv.org/abs/1905.13727)                                                                                                                              | [code](https://github.com/epfml/powersgd)                                                                            | GD    |
| PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization                                             | 2019 | PowerSGDM                                  | [arxiv](https://arxiv.org/abs/1905.13727)                                                                                                                              | [code](https://github.com/epfml/powersgd)                                                                            | GD    |
| On the Variance of the Adaptive Learning Rate and Beyond                                                                   | 2019 | RAdam                                      | [arxiv](https://arxiv.org/abs/1908.03265)                                                                                                                              | [code](https://github.com/LiyuanLucasLiu/RAdam)                                                                      | GD    |
| Matrix-Free Preconditioning in Online Learning                                                                             | 2019 | RecursiveOptimizer                         | [arxiv](https://arxiv.org/abs/1905.12721)                                                                                                                              | [code](https://github.com/google-research/google-research/tree/master/recursive_optimizer)                           | GD    |
| On Empirical Comparisons of Optimizers for Deep Learning                                                                   | 2019 | RMSterov                                   | [arxiv](https://arxiv.org/abs/1910.05446)                                                                                                                              |                                                                                                                      | GD    |
| SAdam: A Variant of Adam for Strongly Convex Functions                                                                     | 2019 | SAdam                                      | [arxiv](https://arxiv.org/abs/1905.02957)                                                                                                                              | [code](https://github.com/SAdam-ICLR2020/codes)                                                                      | GD    |
| Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM                                                      | 2019 | Sadam                                      | [arxiv](https://arxiv.org/abs/1908.00700)                                                                                                                              | [code](https://github.com/neilliang90/Sadam)                                                                         | GD    |
| Calibrating the Adaptive Learning Rate to Improve Convergence of ADAM                                                      | 2019 | SAMSGrad                                   | [arxiv](https://arxiv.org/abs/1908.00700)                                                                                                                              | [code](https://github.com/neilliang90/Sadam)                                                                         | GD    |
| signADAM: Learning Confidences for Deep Neural Networks                                                                    | 2019 | signADAM                                   | [arxiv](https://arxiv.org/abs/1907.09008)                                                                                                                              | [code](https://github.com/DongWanginxdu/signADAM-Learn-by-Confidence)                                                | GD    |
| signADAM: Learning Confidences for Deep Neural Networks                                                                    | 2019 | signADAM++                                 | [arxiv](https://arxiv.org/abs/1907.09008)                                                                                                                              | [code](https://github.com/DongWanginxdu/signADAM-Learn-by-Confidence)                                                | GD    |
| Memory-Efficient Adaptive Optimization                                                                                     | 2019 | SM3                                        | [arxiv](https://arxiv.org/abs/1901.11150)                                                                                                                              | [code](https://github.com/google-research/google-research/tree/master/sm3)                                           | GD    |
| Momentum-Based Variance Reduction in Non-Convex SGD                                                                        | 2019 | STORM                                      | [arxiv](https://arxiv.org/abs/1905.10018)                                                                                                                              | [code](https://github.com/google-research/google-research/tree/master/storm_optimizer)                               | GD    |
| ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization                                                 | 2019 | ZO-AdaMM                                   | [arxiv](https://arxiv.org/abs/1910.06513)                                                                                                                              | [code](https://github.com/KaidiXu/ZO-AdaMM)                                                                          | GF    |
| signSGD via Zeroth-Order Oracle                                                                                            | 2019 | ZO-signSGD                                 | [openreview](https://openreview.net/forum\?id\=BJe-DsC5Fm)                                                                                                             |                                                                                                                      | GF    |
| Demon: Improved Neural Network Training with Momentum Decay                                                                | 2019 | Demon SGDM                                 | [arxiv](https://arxiv.org/abs/1910.04952)                                                                                                                              | [code](https://github.com/autasi/demon_sgd)                                                                          | GD    |
| Demon: Improved Neural Network Training with Momentum Decay                                                                | 2019 | Demon Adam                                 | [arxiv](https://arxiv.org/abs/1910.04952)                                                                                                                              | [code](https://github.com/autasi/demon_sgd)                                                                          | GD    |
| An Optimistic Acceleration of AMSGrad for Nonconvex Optimization                                                           | 2019 | OPT-AMSGrad                                | [arxiv](https://arxiv.org/abs/1903.01435)                                                                                                                              |                                                                                                                      | GD    |
| UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization                             | 2019 | UniXGrad                                   | [arxiv](https://arxiv.org/abs/1910.13857)                                                                                                                              |                                                                                                                      | GD    |
| An Adaptive Optimization Algorithm Based on Hybrid Power and Multidimensional Update Strategy                              | 2019 | AdaHMG                                     | [ieee](https://ieeexplore.ieee.org/abstract/document/8635473)                                                                                                          |                                                                                                                      | GD    |
| ProxSGD: Training Structured Neural Networks under Regularization and Constraints                                          | 2019 | ProxSGD                                    | [openreview](https://openreview.net/forum\?id\=HygpthEtvr)                                                                                                             | [code](https://github.com/optyang/proxsgd)                                                                           | GD    |
| Efficient Learning Rate Adaptation for Convolutional Neural Network Training                                               | 2019 | e-AdLR                                     | [ieee](https://ieeexplore.ieee.org/abstract/document/8852033)                                                                                                          |                                                                                                                      | GD    |
| AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients                                                | 2020 | AdaBelief                                  | [arxiv](https://arxiv.org/abs/2010.07468)                                                                                                                              | [code](https://github.com/juntang-zhuang/Adabelief-Optimizer)                                                        | GD    |
| ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning                                                        | 2020 | ADAHESSIAN                                 | [arxiv](https://arxiv.org/abs/2006.00719)                                                                                                                              | [code](https://github.com/amirgholami/adahessian)                                                                    | GD    |
| Adai: Separating the Effects of Adaptive Learning Rate and Momentum Inertia                                                | 2020 | Adai                                       | [arxiv](https://arxiv.org/abs/2006.15815)                                                                                                                              | [code](https://github.com/zeke-xie/adaptive-inertia-adai)                                                            | GD    |
| Adam<sup>+</sup>: A Stochastic Method with Adaptive Variance Reduction                                                     | 2020 | Adam<sup>+</sup>                           | [arxiv](https://arxiv.org/abs/2011.11985)                                                                                                                              |                                                                                                                      | GD    |
| Adam with Bandit Sampling for Deep Learning                                                                                | 2020 | Adambs                                     | [arxiv](https://arxiv.org/abs/2010.12986)                                                                                                                              | [code](https://github.com/forestliurui/Adam-with-Bandit-Sampling)                                                    | GD    |
| Why are Adaptive Methods Good for Attention Models?                                                                        | 2020 | ACClip                                     | [arxiv](https://arxiv.org/abs/1912.03194)                                                                                                                              |                                                                                                                      | GD    |
| AdamP: Slowing Down the Slowdown for Momentum Optimizers on Scale-invariant Weights                                        | 2020 | AdamP                                      | [arxiv](https://arxiv.org/abs/2006.08217)                                                                                                                              | [code](https://github.com/clovaai/AdamP)                                                                             | GD    |
| On the Trend-corrected Variant of Adaptive Stochastic Optimization Methods                                                 | 2020 | AdamT                                      | [arxiv](https://arxiv.org/abs/2001.06130)                                                                                                                              | [code](https://github.com/xuebin-zh/AdamT)                                                                           | GD    |
| AdaS: Adaptive Scheduling of Stochastic Gradients                                                                          | 2020 | AdaS                                       | [arxiv](https://arxiv.org/abs/2006.06587)                                                                                                                              | [code](https://github.com/mahdihosseini/AdaS)                                                                        | GD    |
| AdaScale SGD: A User-Friendly Algorithm for Distributed Training                                                           | 2020 | AdaScale                                   | [arxiv](https://arxiv.org/abs/2007.05105)                                                                                                                              |                                                                                                                      | GD    |
| AdaSGD: Bridging the gap between SGD and Adam                                                                              | 2020 | AdaSGD                                     | [arxiv](https://arxiv.org/abs/2006.16541)                                                                                                                              |                                                                                                                      | GD    |
| AdaX: Adaptive Gradient Descent with Exponential Long Term Memory                                                          | 2020 | AdaX                                       | [arxiv](https://arxiv.org/abs/2004.09740)                                                                                                                              | [code](https://github.com/switchablenorms/AdaX)                                                                      | GD    |
| AdaX: Adaptive Gradient Descent with Exponential Long Term Memory                                                          | 2020 | AdaX-W                                     | [arxiv](https://arxiv.org/abs/2004.09740)                                                                                                                              | [code](https://github.com/switchablenorms/AdaX)                                                                      | GD    |
| AEGD: Adaptive Gradient Descent with Energy                                                                                | 2020 | AEGD                                       | [arxiv](https://arxiv.org/abs/2010.05109)                                                                                                                              | [code](https://github.com/txping/AEGD)                                                                               | GD    |
| Biased Stochastic Gradient Descent for Conditional Stochastic Optimization                                                 | 2020 | BSGD                                       | [arxiv](https://arxiv.org/abs/2002.10790)                                                                                                                              |                                                                                                                      | GD    |
| Compositional ADAM: An Adaptive Compositional Solver                                                                       | 2020 | C-ADAM                                     | [arxiv](https://arxiv.org/abs/2002.03755)                                                                                                                              |                                                                                                                      | GD    |
| CADA: Communication-Adaptive Distributed Adam                                                                              | 2020 | CADA                                       | [arxiv](https://arxiv.org/abs/2012.15469)                                                                                                                              | [code](https://github.com/ChrisYZZ/CADA-master)                                                                      | GD    |
| CoolMomentum: A Method for Stochastic Optimization by Langevin Dynamics with Simulated Annealing                           | 2020 | CoolMomentum                               | [arxiv](https://arxiv.org/abs/2005.14605)                                                                                                                              | [code](https://github.com/borbysh/coolmomentum)                                                                      | GD    |
| EAdam Optimizer: How  Impact Adam                                                                                         | 2020 | EAdam                                      | [arxiv](https://arxiv.org/abs/2011.02150)                                                                                                                              | [code](https://github.com/yuanwei2019/EAdam-optimizer)                                                               | GD    |
| Expectigrad: Fast Stochastic Optimization with Robust Convergence Properties                                               | 2020 | Expectigrad                                | [arxiv](https://arxiv.org/abs/2010.01356)                                                                                                                              | [code](https://github.com/brett-daley/expectigrad)                                                                   | GD    |
| Stochastic Gradient Descent with Nonlinear Conjugate Gradient-Style Adaptive Momentum                                      | 2020 | FRSGD                                      | [arxiv](https://arxiv.org/abs/2012.02188)                                                                                                                              |                                                                                                                      | GD    |
| Iterative Averaging in the Quest for Best Test Error                                                                       | 2020 | Gadam                                      | [arxiv](https://arxiv.org/abs/2003.01247)                                                                                                                              |                                                                                                                      | GD    |
| A Variant of Gradient Descent Algorithm Based on Gradient Averaging                                                        | 2020 | Grad-Avg                                   | [arxiv](https://arxiv.org/abs/2012.02387)                                                                                                                              |                                                                                                                      | GD    |
| Gravilon: Applications of a New Gradient Descent Method to Machine Learning                                                | 2020 | Gravilon                                   | [arxiv](https://arxiv.org/abs/2008.11370)                                                                                                                              |                                                                                                                      | GD    |
| Practical Quasi-Newton Methods for Training Deep Neural Networks                                                           | 2020 | K-BFGS                                     | [arxiv](https://arxiv.org/abs/2006.08877)                                                                                                                              | [code](https://github.com/renyiryry/kbfgs_neurips2020_public)                                                        | GD    |
| Practical Quasi-Newton Methods for Training Deep Neural Networks                                                           | 2020 | K-BFGS(L)                                  | [arxiv](https://arxiv.org/abs/2006.08877)                                                                                                                              | [code](https://github.com/renyiryry/kbfgs_neurips2020_public)                                                        | GD    |
| LaProp: Separating Momentum and Adaptivity in Adam                                                                         | 2020 | LaProp                                     | [arxiv](https://arxiv.org/abs/2002.04839)                                                                                                                              | [code](https://github.com/Z-T-WANG/LaProp-Optimizer)                                                                 | GD    |
| Mixing ADAM and SGD: a Combined Optimization Method                                                                        | 2020 | MAS                                        | [arxiv](https://arxiv.org/abs/2011.08042)                                                                                                                              | [code](https://gitlab.com/nicolalandro/multi_optimizer)                                                              | GD    |
| Self-Tuning Stochastic Optimization with Curvature-Aware Gradient Filtering                                                | 2020 | MEKA                                       | [arxiv](https://arxiv.org/abs/2011.04803)                                                                                                                              |                                                                                                                      | GD    |
| MTAdam: Automatic Balancing of Multiple Training Loss Terms                                                                | 2020 | MTAdam                                     | [arxiv](https://arxiv.org/abs/2006.14683)                                                                                                                              | [code](https://github.com/ItzikMalkiel/MTAdam)                                                                       | GD    |
| Momentum with Variance Reduction for Nonconvex Composition Optimization                                                    | 2020 | MVRC-1                                     | [arxiv](https://arxiv.org/abs/2005.07755)                                                                                                                              |                                                                                                                      | GD    |
| Momentum with Variance Reduction for Nonconvex Composition Optimization                                                    | 2020 | MVRC-2                                     | [arxiv](https://arxiv.org/abs/2005.07755)                                                                                                                              |                                                                                                                      | GD    |
| PAGE: A Simple and Optimal Probabilistic Gradient Estimator for Nonconvex Optimization                                     | 2020 | PAGE                                       | [arxiv](https://arxiv.org/abs/2008.10898)                                                                                                                              |                                                                                                                      | GD    |
| Momentum-based variance-reduced proximal stochastic gradient method for composite nonconvex stochastic optimization        | 2020 | PSTorm                                     | [arxiv](https://arxiv.org/abs/2006.00425)                                                                                                                              |                                                                                                                      | GD    |
| Ranger-Deep-Learning-Optimizer                                                                                             | 2020 | Ranger                                     | [github](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)                                                                                                  | [code](https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer)                                                  | GD    |
| Gradient Centralization: A New Optimization Technique for Deep Neural Networks                                             | 2020 | GC                                         | [arxiv](https://arxiv.org/abs/2004.01461)                                                                                                                              | [code](https://github.com/Yonghongwei/Gradient-Centralization)                                                       | GD    |
| S-SGD: Symmetrical Stochastic Gradient Descent with Weight Noise Injection for Reaching Flat Minima                        | 2020 | S-SGD                                      | [arxiv](https://arxiv.org/abs/2009.02479)                                                                                                                              |                                                                                                                      | GD    |
| SALR: Sharpness-aware Learning Rate Scheduler for Improved Generalization                                                  | 2020 | SALR                                       | [arxiv](https://arxiv.org/abs/2011.05348)                                                                                                                              |                                                                                                                      | GD    |
| Sharpness-aware Minimization for Efficiently Improving Generalization                                                      | 2020 | SAM                                        | [arxiv](https://arxiv.org/abs/2010.01412)                                                                                                                              | [code](https://github.com/google-research/sam)                                                                       | GD    |
| Stochastic Runge-Kutta methods and adaptive SGD-G2 stochastic gradient descent                                             | 2020 | SGD-G2                                     | [arxiv](https://arxiv.org/abs/2002.09304)                                                                                                                              |                                                                                                                      | GD    |
| A New Accelerated Stochastic Gradient Method with Momentum                                                                 | 2020 | SGDM                                       | [arxiv](https://arxiv.org/abs/2006.00423)                                                                                                                              |                                                                                                                      | GD    |
| Scheduled Restart Momentum for Accelerated Stochastic Gradient Descent                                                     | 2020 | SRSGD                                      | [arxiv](https://arxiv.org/abs/2002.10583)                                                                                                                              | [code](https://github.com/minhtannguyen/SRSGD)                                                                       | GD    |
| Adaptive Gradient Methods Can Be Provably Faster than SGD after Finite Epochs                                              | 2020 | SHAdaGrad                                  | [arxiv](https://arxiv.org/abs/2006.07037)                                                                                                                              |                                                                                                                      | GD    |
| Enhance Curvature Information by Structured Stochastic Quasi-Newton Methods                                                | 2020 | SKQN                                       | [arxiv](https://arxiv.org/abs/2006.09606)                                                                                                                              |                                                                                                                      | GD    |
| Enhance Curvature Information by Structured Stochastic Quasi-Newton Methods                                                | 2020 | S4QN                                       | [arxiv](https://arxiv.org/abs/2006.09606)                                                                                                                              |                                                                                                                      | GD    |
| SMG: A Shuffling Gradient-Based Method with Momentum                                                                       | 2020 | SMG                                        | [arxiv](https://arxiv.org/abs/2011.11884)                                                                                                                              |                                                                                                                      | GD    |
| Stochastic Normalized Gradient Descent with Momentum for Large Batch Training                                              | 2020 | SNGM                                       | [arxiv](https://arxiv.org/abs/2007.13985)                                                                                                                              |                                                                                                                      | GD    |
| TAdam: A Robust Stochastic Gradient Optimizer                                                                              | 2020 | TAdam                                      | [arxiv](https://arxiv.org/abs/2003.00179)                                                                                                                              | [code](https://github.com/Mahoumaru/TAdam)                                                                           | GD    |
| Eigenvalue-corrected Natural Gradient Based on a New Approximation                                                         | 2020 | TEKFAC                                     | [arxiv](https://arxiv.org/abs/2011.13609)                                                                                                                              |                                                                                                                      | GD    |
| pbSGD: Powered Stochastic Gradient Descent Methods for Accelerated Non-Convex Optimization                                 | 2020 | pbSGD                                      | [ijcai](https://www.ijcai.org/proceedings/2020/451)                                                                                                                    | [code](https://github.com/HAIRLAB/pbSGD)                                                                             | GD    |
| Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization                      | 2020 | Apollo                                     | [arxiv](https://arxiv.org/abs/2009.13586)                                                                                                                              | [code](https://github.com/XuezheMax/apollo)                                                                          | GD, S |
| Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization                      | 2020 | ApolloW                                    | [arxiv](https://arxiv.org/abs/2009.13586)                                                                                                                              | [code](https://github.com/XuezheMax/apollo)                                                                          | GD, S |
| Slime mould algorithm: A new method for stochastic optimization                                                            | 2020 | SMA                                        | [sciencedirect](https://www.sciencedirect.com/science/article/abs/pii/S0167739X19320941)                                                                               | [code](http://mdm.wzu.edu.cn/SMA.html)                                                                               | E     |
| AdaSwarm: Augmenting Gradient-Based optimizers in Deep Learning with Swarm Intelligence                                    | 2020 | AdaSwarm                                   | [arxiv](https://arxiv.org/abs/2006.09875)                                                                                                                              | [code](https://github.com/anuwu/PSO-Stuff)                                                                           | E     |
| Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities                                 | 2020 | AdaACSA                                    | [arxiv](https://arxiv.org/abs/2007.08840)                                                                                                                              |                                                                                                                      | GD    |
| Adaptive Gradient Methods for Constrained Convex Optimization and Variational Inequalities                                 | 2020 | AdaAGD+                                    | [arxiv](https://arxiv.org/abs/2007.08840)                                                                                                                              |                                                                                                                      | GD    |
| SCW-SGD: Stochastically Confidence-Weighted SGD                                                                            | 2020 | SCWSGD                                     | [ieee](https://ieeexplore.ieee.org/abstract/document/9190992)                                                                                                          |                                                                                                                      | GD    |
| An Improved Adaptive Optimization Technique for Image Classification                                                       | 2020 | Mean-ADAM                                  | [ieee](https://ieeexplore.ieee.org/abstract/document/9306620)                                                                                                          |                                                                                                                      | GD    |
| Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes                                                     | 2020 | LANS                                       | [arxiv](https://arxiv.org/abs/2006.13484)                                                                                                                              | [code](https://github.com/jianyuheng/lans_optimizer)                                                                 | GD    |
| Weak and Strong Gradient Directions: Explaining Memorization, Generalization, and Hardness of Examples at Scale            | 2020 | RM3                                        | [arxiv](https://arxiv.org/abs/2003.07422)                                                                                                                              | [code](https://github.com/cpuimage/keras-optimizer)                                                                  | GD    |
| On the distance between two neural networks and the stability of learning                                                  | 2020 | Fromage                                    | [arxiv](https://arxiv.org/abs/2002.03432)                                                                                                                              | [code](https://github.com/jxbz/fromage)                                                                              | GD    |
| Towards Better Generalization of Adaptive Gradient Methods                                                                 | 2020 | SAGD                                       | [neurips](https://proceedings.neurips.cc/paper/2020/hash/08fb104b0f2f838f3ce2d2b3741a12c2-Abstract.html)                                                               |                                                                                                                      | GD    |
| Smooth momentum: improving lipschitzness in gradient descent                                                               | 2022 | Smooth Momentum                            | [springerlink](https://link.springer.com/article/10.1007/s10489-022-04207-7)                                                                                           |                                                                                                                      | GD    |
| Grad-GradaGrad? A Non-Monotone Adaptive Stochastic Gradient Method                                                         | 2022 | GradaGrad                                  | [arxiv](https://arxiv.org/abs/2206.06900)                                                                                                                              |                                                                                                                      | GD    |
| VeLO: Training Versatile Learned Optimizers by Scaling Up                                                                  | 2022 | VeLO                                       | [arxiv](https://arxiv.org/abs/2211.09760)                                                                                                                              | [Jax](https://github.com/google/learned_optimization/tree/main/learned_optimization/research/general_lopt)           | GD    |
| CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU                                  | 2022 | CowClip                                    | [arxiv](https://arxiv.org/pdf/2204.06240.pdf)                                                                                                                          | [TF](https://github.com/bytedance/LargeBatchCTR)                                                                     | GD    |
| Amos: An Adam-style Optimizer with Adaptive Weight Decay towards Model-Oriented Scale                                      | 2022 | Amos                                       | [arxiv](https://arxiv.org/pdf/2210.11693.pdf)                                                                                                                          | [Jax](https://github.com/google-research/jestimator)                                                                 | GD    |
| Symbolic Discovery of Optimization Algorithms                                                                              | 2023 | Lion                                       | [Neurips'23](https://arxiv.org/abs/2302.06675)                                                                                                                         | [Jax, TF, PyTorch](https://github.com/google/automl/tree/master/lion)                                                | GD    |
| FOSI: Hybrid First and Second Order Optimization                                                                           | 2023 | FOSI                                       | [HPI'23](https://arxiv.org/abs/2302.08484)                                                                                                                             | [Jax](https://github.com/hsivan/fosi)                                                                                | GD, S |
| DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule                                                      | 2023 | DoG                                        | [ICML'23](https://arxiv.org/abs/2302.12022)                                                                                                                            | [PyTorch](https://github.com/formll/dog)                                                                             | GD    |
| An Adam-enhanced Particle Swarm Optimizer for Latent Factor Analysis                                                       | 2023 | ADHPL                                      | [arxiv](https://arxiv.org/abs/2302.11956)                                                                                                                              |                                                                                                                      | E     |
| DP-Adam: Correcting DP Bias in Adam's Second Moment Estimation                                                             | 2023 | DP-Adam                                    | [ICLR-W'23](https://arxiv.org/abs/2304.11208)                                                                                                                          |                                                                                                                      | GD    |
| Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term                                        | 2023 | WSAM                                       | [KDD'23](https://arxiv.org/abs/2305.15817)                                                                                                                             | [PyTorch](https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers)              | GD    |
| UAdam: Unified Adam-Type Algorithmic Framework for Non-Convex Stochastic Optimization                                      | 2023 | UAdam                                      | [arxiv](https://arxiv.org/abs/2305.05675)                                                                                                                              |                                                                                                                      | GD    |
| Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training                                       | 2023 | Sophia                                     | [arxiv](https://arxiv.org/abs/2305.14342)                                                                                                                              | [PyTorch](https://github.com/Liuhong99/Sophia)                                                                       | GD    |
| DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method                                              | 2023 | DoWG                                       | [Neurips'23](https://arxiv.org/abs/2305.16284)                                                                                                                         |                                                                                                                      | GD    |
| Prodigy: An Expeditiously Adaptive Parameter-Free Learner                                                                  | 2023 | Prodigy                                    | [arxiv](https://arxiv.org/abs/2306.06101)                                                                                                                              | [PyTorch](https://github.com/konstmish/prodigy)                                                                      | GD    |
| CAME: Confidence-guided Adaptive Memory Efficient Optimization                                                             | 2023 | CAME                                       | [ACL'23](https://arxiv.org/abs/2307.02047)                                                                                                                             | [PyTorch](https://github.com/yangluo7/CAME)                                                                          | GD    |
| Promoting Exploration in Memory-Augmented Adam using Critical Momenta                                                      | 2023 | Adam+CM                                    | [arxiv](https://arxiv.org/abs/2307.09638)                                                                                                                              | [PyTorch](https://github.com/chandar-lab/CMOptimizer)                                                                | GD    |
| Large Language Models as Optimizers                                                                                        | 2023 | OPRO                                       | [arxiv](https://arxiv.org/abs/2309.03409)                                                                                                                              | [python](https://github.com/google-deepmind/opro)                                                                    | llm   |
| AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix                            | 2023 | AGD                                        | [Neurips'23](https://arxiv.org/abs/2312.01658)                                                                                                                         | [PyTorch](https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers)              | GD, S |
